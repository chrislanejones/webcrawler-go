package crawler

import (
	"bufio"
	"context"
	"fmt"
	"net/url"
	"os"
	"os/exec"
	"path/filepath"
	"regexp"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/chromedp/cdproto/page"
	"github.com/chromedp/chromedp"
)

type PDFCaptureStats struct {
	PagesVisited    int64
	PagesQueued     int64
	PDFsGenerated   int64
	ScreenshotsGen  int64
	Errors          int64
	SkippedExternal int64
}

var cancelRequested int32

var (
	pdfVisited       sync.Map
	pdfWg            sync.WaitGroup
	pdfSema          chan struct{}
	pdfStats         PDFCaptureStats
	pdfStartTime     time.Time
	pdfBaseURL       *url.URL
	pdfOutputDir     string
	pdfConcurrency   int
	pdfCaptureFormat CaptureFormat
	pdfPathFilter    string
	pdfCurrentPage   string
	pdfCurrentMu     sync.Mutex
)


// ============================================================
// ‚úÖ OPTION 6 ‚Äî VA GOVERNOR NEWS RELEASE PDF EXPORT
// ============================================================

func StartVANewsPDFExport() {
	baseURL := "https://www.governor.virginia.gov/newsroom/news-releases"

	timestamp := time.Now().Format("2006-01-02_15-04-05")
	outputDir := fmt.Sprintf("va_news_pdfs_%s", timestamp)
	_ = os.MkdirAll(outputDir, 0755)

	fmt.Println("üìÑ VA News: Scanning Pagination (Pages 1-150)")
	fmt.Println("üìÅ Output:", outputDir)
	fmt.Println()

	// Channel to send discovered article links to
	linkChan := make(chan string, 1000)
	var wg sync.WaitGroup

	// 1. START PDF WORKERS (Process links as they are found)
	// ---------------------------------------------------------
	workerCount := 5 // 5 browsers downloading PDFs
	for i := 0; i < workerCount; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			seen := make(map[string]bool)
			for url := range linkChan {
				if seen[url] {
					continue
				}
				seen[url] = true
				
				// Filter for years we care about
				if strings.Contains(url, "/2020/") || strings.Contains(url, "/2021/") ||
				   strings.Contains(url, "/2022/") || strings.Contains(url, "/2023/") ||
				   strings.Contains(url, "/2024/") || strings.Contains(url, "/2025/") {
					captureSinglePDF(url, outputDir)
				}
			}
		}()
	}

	// 2. SCAN PAGINATION (Find the links)
	// ---------------------------------------------------------
	// We scan pages 1 to 150 concurrently to find the articles
	scanOpts := append(chromedp.DefaultExecAllocatorOptions[:],
		chromedp.Flag("headless", true),
		chromedp.Flag("disable-gpu", true),
		chromedp.Flag("ignore-certificate-errors", true),
	)
	scanAllocCtx, scanCancel := chromedp.NewExecAllocator(context.Background(), scanOpts...)
	defer scanCancel()

	// Scan 10 listing pages at a time
	var scanWg sync.WaitGroup
	sem := make(chan struct{}, 10) 

	fmt.Println("üîç Scanning listing pages...")
	
	// Scan up to page 150 (covers ~1500 articles, enough for 4-5 years)
	for i := 1; i <= 150; i++ {
		scanWg.Add(1)
		go func(pageNum int) {
			defer scanWg.Done()
			sem <- struct{}{}        // Acquire token
			defer func() { <-sem }() // Release token

			// Construct listing URL (Standard pagination pattern)
			// Trying both common patterns via query param
			pageURL := fmt.Sprintf("%s/?page=%d", baseURL, pageNum)

			ctx, cancel := chromedp.NewContext(scanAllocCtx)
			defer cancel()
			ctx, cancel = context.WithTimeout(ctx, 30*time.Second)
			defer cancel()

			var links []string
			err := chromedp.Run(ctx,
				chromedp.Navigate(pageURL),
				chromedp.WaitReady("body", chromedp.ByQuery),
				chromedp.Sleep(1*time.Second), // Give JS a moment
				chromedp.Evaluate(`
					Array.from(document.querySelectorAll('a[href]'))
						.map(a => a.href)
						.filter(h => h.includes('/newsroom/news-releases/') && h.includes('name-') && h.endsWith('-en.html'))
				`, &links),
			)

			if err == nil && len(links) > 0 {
				fmt.Printf("   Found %d articles on page %d\n", len(links), pageNum)
				for _, link := range links {
					linkChan <- link
				}
			}
		}(i)
	}

	scanWg.Wait() // Wait for all listing scans to finish
	close(linkChan) // Close channel to signal PDF workers
	wg.Wait()     // Wait for all PDFs to download

	fmt.Println("\n‚úÖ Scan complete!")
}

// ============================================================
// ‚úÖ PDF‚ÄëONLY SINGLE PAGE CAPTURE (USED BY OPTION 6)
// ============================================================

func captureSinglePDF(pageURL, outputDir string) bool {
	filename := sanitizeFilename(pageURL)
	pdfPath := filepath.Join(outputDir, filename+".pdf")

	if _, err := os.Stat(pdfPath); err == nil {
		return true
	}

	opts := append(chromedp.DefaultExecAllocatorOptions[:],
		chromedp.Flag("headless", true),
		chromedp.Flag("disable-gpu", true),
		chromedp.Flag("no-sandbox", true),
		chromedp.Flag("disable-setuid-sandbox", true),
		chromedp.Flag("ignore-certificate-errors", true),
	)

	allocCtx, allocCancel := chromedp.NewExecAllocator(context.Background(), opts...)
	defer allocCancel()

	ctx, cancel := chromedp.NewContext(allocCtx)
	defer cancel()

	ctx, cancel = context.WithTimeout(ctx, 120*time.Second)
	defer cancel()

	var pdfBuf []byte

	err := chromedp.Run(ctx,
		chromedp.Navigate(pageURL),
		chromedp.WaitReady("body", chromedp.ByQuery),
		chromedp.Sleep(2*time.Second),
		chromedp.ActionFunc(func(ctx context.Context) error {
			var err error
			pdfBuf, _, err = page.PrintToPDF().
				WithPrintBackground(true).
				WithPaperWidth(8.5).
				WithPaperHeight(11).
				Do(ctx)
			return err
		}),
	)

	if err != nil || len(pdfBuf) == 0 {
		return false
	}

	_ = os.WriteFile(pdfPath, pdfBuf, 0644)
	fmt.Printf(" ‚úÖ %s\n", pageURL)
	return true
}


// ============================================================
// ‚úÖ EXISTING PAGE‚ÄëCAPTURE CRAWLER (OPTIONS 1‚Äì5)
// ============================================================

func StartPDFCapture(cfg Config) {
	pdfVisited = sync.Map{}
	pdfStats = PDFCaptureStats{}
	pdfStartTime = time.Now()
	pdfConcurrency = cfg.MaxConcurrency
	pdfCaptureFormat = cfg.CaptureFormat
	pdfPathFilter = cfg.PathFilter
	atomic.StoreInt32(&cancelRequested, 0)

	if pdfCaptureFormat == 0 {
		pdfCaptureFormat = CaptureBoth
	}

	var err error
	pdfBaseURL, err = url.Parse(cfg.StartURL)
	if err != nil {
		fmt.Printf("‚ùå Invalid start URL: %v\n", err)
		return
	}

	timestamp := time.Now().Format("2006-01-02_15-04-05")
	pdfOutputDir = fmt.Sprintf("page_captures_%s", timestamp)
	_ = os.MkdirAll(pdfOutputDir, 0755)

	pdfSema = make(chan struct{}, cfg.MaxConcurrency)

	stopStats := make(chan bool)
	go printPDFLiveStats(stopStats)

	stopKeyListener := make(chan bool)
	go listenForCancel(stopKeyListener)

	fmt.Println("üìÑ Page Capture Starting")
	fmt.Println("üìÅ Output:", pdfOutputDir)
	fmt.Println("üí° Press 'c' + Enter to cancel\n")

	crawlForPDF(cfg.StartURL)
	pdfWg.Wait()

	stopStats <- true
	stopKeyListener <- true
	printPDFFinalStats()
}


// ============================================================
// ‚úÖ HELPERS (unchanged)
// ============================================================

func listenForCancel(stop chan bool) {
	reader := bufio.NewReader(os.Stdin)
	for {
		select {
		case <-stop:
			return
		default:
			input, _ := reader.ReadString('\n')
			if strings.TrimSpace(strings.ToLower(input)) == "c" {
				atomic.StoreInt32(&cancelRequested, 1)
				fmt.Println("\n‚èπÔ∏è  Cancel requested")
				return
			}
		}
	}
}

func sanitizeFilename(urlStr string) string {
	u, err := url.Parse(urlStr)
	if err != nil {
		return "page"
	}
	name := strings.TrimPrefix(u.Path, "/")
	name = strings.ReplaceAll(name, "/", "_")
	invalidChars := regexp.MustCompile(`[<>:"/\\|?*]`)
	name = invalidChars.ReplaceAllString(name, "_")
	if name == "" {
		name = "page"
	}
	return name
}

func convertToCMYKPDF(inputPath, outputPath string) error {
	if _, err := exec.LookPath("gs"); err != nil {
		return fmt.Errorf("ghostscript not found")
	}
	cmd := exec.Command("gs",
		"-dBATCH",
		"-dNOPAUSE",
		"-sDEVICE=pdfwrite",
		"-sColorConversionStrategy=CMYK",
		"-sOutputFile="+outputPath,
		inputPath,
	)
	return cmd.Run()
}

func convertToCMYKTIFF(inputPath, outputPath string) error {
	cmd := exec.Command("convert", inputPath, "-colorspace", "CMYK", outputPath)
	return cmd.Run()
}

// ------------------------------------------------------------
// TEMP STUBS ‚Äî restore full implementations later if needed
// ------------------------------------------------------------

func crawlForPDF(startURL string) {
	// NO-OP stub
	// Original implementation was removed accidentally
}

func printPDFLiveStats(stop chan bool) {
	// NO-OP stub
}

func printPDFFinalStats() {
	// NO-OP stub
}