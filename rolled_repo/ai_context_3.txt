
===== FILE: internal/parser/docx.go =====
package parser
import (
"bytes"
"io"
"strings"
"baliance.com/gooxml/document"
)
func ContainsLinkInDocx(r io.Reader, target string) bool {
buf, err := io.ReadAll(r)
if err != nil {
return false
}
reader := bytes.NewReader(buf)
doc, err := document.Read(reader, int64(len(buf)))
if err != nil {
return false
}
for _, para := range doc.Paragraphs() {
for _, run := range para.Runs() {
if strings.Contains(run.Text(), target) {
return true
}
}
}
return false
}

===== FILE: internal/parser/pdf.go =====
package parser
import (
"io"
"os"
"os/exec"
"path/filepath"
"strings"
)
func ContainsLinkInPDF(r io.Reader, target string) bool {
buf, err := io.ReadAll(r)
if err != nil {
return false
}
os.MkdirAll("assets/tmp", 0755)
tmpPDF := "assets/tmp/tmp.pdf"
if err := os.WriteFile(tmpPDF, buf, 0644); err != nil {
return false
}
outDir := "assets/tmp/text"
os.MkdirAll(outDir, 0755)
cmd := exec.Command("pdfcpu", "extract", "-mode", "text", tmpPDF, outDir)
if err := cmd.Run(); err != nil {
os.RemoveAll(outDir)
os.Remove(tmpPDF)
return false
}
found := false
filepath.Walk(outDir, func(path string, info os.FileInfo, err error) error {
if err != nil {
return nil
}
if strings.HasSuffix(path, ".txt") {
data, readErr := os.ReadFile(path)
if readErr == nil && strings.Contains(string(data), target) {
found = true
}
}
return nil
})
os.RemoveAll(outDir)
os.Remove(tmpPDF)
return found
}

===== FILE: LICENSE =====
MIT License
Copyright (c) 2025 Chris Lane Jones
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

===== FILE: main.go =====
package main
import (
"bufio"
"crypto/tls"
"fmt"
"net/http"
"net/url"
"os"
"strconv"
"strings"
"time"
"webcrawler/internal/crawler"
)
func main() {
reader := bufio.NewReader(os.Stdin)
fmt.Println()
fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•—")
fmt.Println("â•‘ ğŸ•·ï¸ Web Crawler Wizard ğŸ•·ï¸ â•‘")
fmt.Println("â•‘ v2.1 - Cloudflare Buster â•‘")
fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•")
fmt.Println()
var siteURL string
var altEntryPoints []string
var pathFilter string
for {
fmt.Print("ğŸŒ What site do you want to check?\n (Tip: Include a path like /newsroom/ to only crawl that section)\n â†’ ")
input, err := reader.ReadString('\n')
if err != nil {
fmt.Println("âŒ Error reading input:", err)
continue
}
siteURL = strings.TrimSpace(input)
if !strings.HasPrefix(siteURL, "http:
siteURL = "https:
}
parsedURL, err := url.Parse(siteURL)
if err != nil || parsedURL.Host == "" {
fmt.Println("âŒ Invalid URL. Please enter a valid website address.")
continue
}
if parsedURL.Path != "" && parsedURL.Path != "/" {
pathFilter = parsedURL.Path
if !strings.HasSuffix(pathFilter, "/") {
pathFilter = pathFilter + "/"
}
fmt.Printf("\n ğŸŒ² Detected path: %s\n", pathFilter)
fmt.Print(" ğŸ“ Only crawl pages under this path? (Y/n): ")
confirmPath, _ := reader.ReadString('\n')
confirmPath = strings.ToLower(strings.TrimSpace(confirmPath))
if confirmPath == "n" || confirmPath == "no" {
pathFilter = ""
fmt.Println(" âœ“ Will crawl entire site")
} else {
fmt.Printf(" âœ“ Will only crawl pages under %s\n", pathFilter)
}
}
fmt.Printf("\nğŸ” Testing connection to %s...\n", siteURL)
success, attempts, blocked := testConnectionWithRetry(siteURL, 3)
if success {
fmt.Printf(" ğŸ“Š Connected after %d attempt(s)\n", attempts)
break
}
if blocked {
fmt.Println()
fmt.Println(" ğŸ›¡ï¸ Cloudflare/Bot protection detected on main page!")
fmt.Println(" ğŸ’¡ Let's try some alternative entry points...")
fmt.Println()
altEntryPoints = suggestAndTestAlternatives(siteURL, reader)
if len(altEntryPoints) > 0 {
fmt.Printf("\n âœ… Found %d working entry point(s)!\n", len(altEntryPoints))
fmt.Println(" ğŸ”„ Will start from these and retry blocked pages later")
break
} else {
fmt.Println("\n ğŸ˜” No alternative entry points worked")
}
}
fmt.Print("\nâš ï¸ Connection issues detected. Try anyway? (y/n): ")
confirm, _ := reader.ReadString('\n')
if strings.ToLower(strings.TrimSpace(confirm)) == "y" {
break
}
}
fmt.Println()
fmt.Println("ğŸ“‹ What should I check the site for?")
fmt.Println()
fmt.Println(" â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
fmt.Println(" â”‚ 1. ğŸ”— Find a link on site (HTML, Word, PDF) â”‚")
fmt.Println(" â”‚ 2. ğŸ“ Find a word/phrase on site (HTML, Word, PDF) â”‚")
fmt.Println(" â”‚ 3. ğŸ’” Search for broken links â”‚")
fmt.Println(" â”‚ 4. ğŸ–¼ï¸ Search for oversized images â”‚")
fmt.Println(" â”‚ 5. ğŸ“„ Generate PDF/Image for every page â”‚")
fmt.Println(" â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
fmt.Println()
var mode crawler.SearchMode
for {
fmt.Print(" Enter choice (1-5): ")
input, err := reader.ReadString('\n')
if err != nil {
fmt.Println("âŒ Error reading input:", err)
continue
}
choice, err := strconv.Atoi(strings.TrimSpace(input))
if err != nil || choice < 1 || choice > 5 {
fmt.Println(" âŒ Please enter a number between 1 and 5")
continue
}
mode = crawler.SearchMode(choice)
break
}
fmt.Println()
var searchTarget string
var imageSizeThreshold int64 = 500
var captureFormat crawler.CaptureFormat = crawler.CaptureBoth
switch mode {
case crawler.ModeSearchLink:
fmt.Print("ğŸ”— Enter the link to search for:\n â†’ ")
input, _ := reader.ReadString('\n')
searchTarget = strings.TrimSpace(input)
if searchTarget == "" {
fmt.Println("âŒ Link cannot be empty")
os.Exit(1)
}
case crawler.ModeSearchWord:
fmt.Print("ğŸ“ Enter the word or phrase to search for:\n â†’ ")
input, _ := reader.ReadString('\n')
searchTarget = strings.TrimSpace(input)
if searchTarget == "" {
fmt.Println("âŒ Search term cannot be empty")
os.Exit(1)
}
case crawler.ModeBrokenLinks:
fmt.Println("ğŸ’” Will search for broken links (404s, timeouts, connection errors)")
case crawler.ModeOversizedImages:
fmt.Print("ğŸ–¼ï¸ Enter max image size in KB (default 500): ")
input, _ := reader.ReadString('\n')
input = strings.TrimSpace(input)
if input != "" {
if size, err := strconv.ParseInt(input, 10, 64); err == nil && size > 0 {
imageSizeThreshold = size
}
}
fmt.Printf(" Looking for images larger than %dKB\n", imageSizeThreshold)
case crawler.ModePDFCapture:
fmt.Println("ğŸ“„ What format do you want to capture?")
fmt.Println()
fmt.Println(" â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
fmt.Println(" â”‚ a. ğŸ“‘ PDF only â”‚")
fmt.Println(" â”‚ b. ğŸ–¼ï¸ Images only (PNG) â”‚")
fmt.Println(" â”‚ c. ğŸ“‘ğŸ–¼ï¸ Both PDF + Images â”‚")
fmt.Println(" â”‚ d. ğŸ¨ CMYK PDF (for print) * â”‚")
fmt.Println(" â”‚ e. ğŸ¨ CMYK TIFF (for InDesign) * â”‚")
fmt.Println(" â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
fmt.Println(" * Requires Ghostscript (d) or ImageMagick (e) installed")
fmt.Println()
for {
fmt.Print(" Enter choice (a/b/c/d/e): ")
formatInput, _ := reader.ReadString('\n')
formatChoice := strings.ToLower(strings.TrimSpace(formatInput))
switch formatChoice {
case "a":
captureFormat = crawler.CapturePDFOnly
fmt.Println(" ğŸ“‘ Will generate PDFs only")
case "b":
captureFormat = crawler.CaptureImagesOnly
fmt.Println(" ğŸ–¼ï¸ Will generate PNG screenshots only")
case "c":
captureFormat = crawler.CaptureBoth
fmt.Println(" ğŸ“‘ğŸ–¼ï¸ Will generate both PDFs and PNG screenshots")
case "d":
captureFormat = crawler.CaptureCMYKPDF
fmt.Println(" ğŸ¨ Will generate CMYK PDFs (requires Ghostscript)")
case "e":
captureFormat = crawler.CaptureCMYKTIFF
fmt.Println(" ğŸ¨ Will generate CMYK TIFFs (requires ImageMagick)")
default:
fmt.Println(" âŒ Please enter a, b, c, d, or e")
continue
}
break
}
fmt.Println(" ğŸ“ Output folder: ./page_captures/")
}
fmt.Println()
fmt.Print("âš¡ Max concurrent requests (default 5, max 20): ")
concurrencyInput, _ := reader.ReadString('\n')
concurrency := 5
if c, err := strconv.Atoi(strings.TrimSpace(concurrencyInput)); err == nil && c > 0 {
if c > 20 {
c = 20
fmt.Println(" âš ï¸ Capped at 20 to avoid getting banned")
}
concurrency = c
}
fmt.Println()
fmt.Print("ğŸ”„ Max retries per page (default 3): ")
retryInput, _ := reader.ReadString('\n')
maxRetries := 3
if r, err := strconv.Atoi(strings.TrimSpace(retryInput)); err == nil && r >= 0 {
maxRetries = r
}
fmt.Println()
fmt.Println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•")
fmt.Println()
config := crawler.Config{
StartURL: siteURL,
AltEntryPoints: altEntryPoints,
Mode: mode,
SearchTarget: searchTarget,
MaxConcurrency: concurrency,
ImageSizeThreshold: imageSizeThreshold * 1024,
MaxRetries: maxRetries,
RetryDelay: 2 * time.Second,
RetryBlockedPages: true,
BlockedRetryPasses: 3,
CaptureFormat: captureFormat,
PathFilter: pathFilter,
}
fmt.Println("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LAUNCH CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
fmt.Printf("â”‚ ğŸŒ Target: %-35s â”‚\n", truncateString(siteURL, 35))
if pathFilter != "" {
fmt.Printf("â”‚ ğŸŒ² Path filter: %-35s â”‚\n", truncateString(pathFilter, 35))
}
fmt.Printf("â”‚ ğŸ“Š Mode: %-35s â”‚\n", mode.String())
if searchTarget != "" {
fmt.Printf("â”‚ ğŸ¯ Search for: %-35s â”‚\n", truncateString(searchTarget, 35))
}
fmt.Printf("â”‚ âš¡ Concurrency: %-35d â”‚\n", concurrency)
fmt.Printf("â”‚ ğŸ”„ Max retries: %-35d â”‚\n", maxRetries)
if len(altEntryPoints) > 0 {
fmt.Printf("â”‚ ğŸšª Alt entries: %-35d â”‚\n", len(altEntryPoints))
}
fmt.Println("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
fmt.Println()
fmt.Println("ğŸš€ LAUNCHING CRAWLER...")
fmt.Println()
crawler.Start(config)
fmt.Println()
fmt.Println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•")
fmt.Println("ğŸ‰ Crawl complete! Check the results CSV file for details.")
fmt.Println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•")
}
func suggestAndTestAlternatives(siteURL string, reader *bufio.Reader) []string {
parsedURL, _ := url.Parse(siteURL)
baseURL := fmt.Sprintf("%s:
commonPaths := []string{
"/about", "/about-us", "/contact", "/contact-us",
"/sitemap.xml", "/robots.txt", "/privacy", "/privacy-policy",
"/terms", "/help", "/faq", "/blog", "/news",
"/products", "/services", "/team", "/careers",
}
fmt.Println(" Testing common entry points...")
fmt.Println()
var workingEntries []string
for i, path := range commonPaths {
testURL := baseURL + path
fmt.Printf(" [%2d/%d] Testing %-20s", i+1, len(commonPaths), path)
success, blocked := quickTest(testURL)
if success {
fmt.Println(" âœ… WORKS!")
workingEntries = append(workingEntries, testURL)
} else if blocked {
fmt.Println(" ğŸ›¡ï¸ Blocked")
} else {
fmt.Println(" âŒ Failed")
}
time.Sleep(500 * time.Millisecond)
}
fmt.Println()
fmt.Print(" ğŸ”§ Enter a custom path to try (or press Enter to skip): ")
customPath, _ := reader.ReadString('\n')
customPath = strings.TrimSpace(customPath)
if customPath != "" {
if !strings.HasPrefix(customPath, "/") {
customPath = "/" + customPath
}
testURL := baseURL + customPath
fmt.Printf(" Testing %s...", customPath)
success, _ := quickTest(testURL)
if success {
fmt.Println(" âœ… WORKS!")
workingEntries = append(workingEntries, testURL)
} else {
fmt.Println(" âŒ Failed")
}
}
return workingEntries
}
func quickTest(testURL string) (success bool, blocked bool) {
client := &http.Client{
Timeout: 10 * time.Second,
Transport: &http.Transport{
TLSClientConfig: &tls.Config{InsecureSkipVerify: true},
},
}
req, err := http.NewRequest("GET", testURL, nil)
if err != nil {
return false, false
}
req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
req.Header.Set("Accept-Language", "en-US,en;q=0.5")
req.Header.Set("Accept-Encoding", "gzip, deflate, br")
req.Header.Set("Connection", "keep-alive")
req.Header.Set("Upgrade-Insecure-Requests", "1")
startTime := time.Now()
resp, err := client.Do(req)
latency := time.Since(startTime)
if err != nil {
errStr := err.Error()
switch {
case strings.Contains(errStr, "timeout"):
fmt.Printf(" â±ï¸ TIMEOUT (%.1fs)\n", latency.Seconds())
case strings.Contains(errStr, "connection refused"):
fmt.Printf(" ğŸš« CONNECTION REFUSED\n")
case strings.Contains(errStr, "no such host"):
fmt.Printf(" ğŸŒ DNS ERROR - Domain not found\n")
return false, attempt, false
case strings.Contains(errStr, "certificate"):
fmt.Printf(" ğŸ”’ SSL ERROR (will skip verification)\n")
default:
fmt.Printf(" âŒ %v\n", err)
}
if attempt < maxAttempts {
delay := time.Duration(attempt*2) * time.Second
fmt.Printf(" â³ Waiting %.0fs before retry...\n", delay.Seconds())
time.Sleep(delay)
}
continue
}
defer resp.Body.Close()
if resp.StatusCode == 403 || resp.StatusCode == 503 {
wasBlocked = true
body := make([]byte, 4096)
n, _ := resp.Body.Read(body)
bodyStr := strings.ToLower(string(body[:n]))
if strings.Contains(bodyStr, "cloudflare") {
fmt.Printf(" ğŸ›¡ï¸ CLOUDFLARE DETECTED (%d)\n", resp.StatusCode)
} else if strings.Contains(bodyStr, "ddos protection") {
fmt.Printf(" ğŸ›¡ï¸ DDOS PROTECTION (%d)\n", resp.StatusCode)
} else {
fmt.Printf(" ğŸ›¡ï¸ BLOCKED (%d)\n", resp.StatusCode)
}
if attempt < maxAttempts {
delay := time.Duration(attempt*3) * time.Second
fmt.Printf(" â³ Waiting %.0fs before retry with different headers...\n", delay.Seconds())
time.Sleep(delay)
}
continue
}
if resp.StatusCode == 429 {
wasBlocked = true
fmt.Printf(" ğŸŒ RATE LIMITED (429)\n")
if attempt < maxAttempts {
delay := time.Duration(attempt*5) * time.Second
fmt.Printf(" â³ Rate limited! Waiting %.0fs...\n", delay.Seconds())
time.Sleep(delay)
}
continue
}
if resp.StatusCode == 200 {
body := make([]byte, 4096)
n, _ := resp.Body.Read(body)
bodyStr := strings.ToLower(string(body[:n]))
if strings.Contains(bodyStr, "checking your browser") ||
(strings.Contains(bodyStr, "please wait") && strings.Contains(bodyStr, "redirecting")) {
wasBlocked = true
fmt.Printf(" ğŸ›¡ï¸ CHALLENGE PAGE DETECTED\n")
if attempt < maxAttempts {
delay := time.Duration(attempt*3) * time.Second
fmt.Printf(" â³ Waiting %.0fs...\n", delay.Seconds())
time.Sleep(delay)
}
continue
}
}
if resp.StatusCode >= 200 && resp.StatusCode < 400 {
fmt.Printf(" âœ… OK (%d) - %.0fms latency\n", resp.StatusCode, float64(latency.Milliseconds()))
return true, attempt, false
}
fmt.Printf(" âš ï¸ Status %d\n", resp.StatusCode)
if attempt < maxAttempts {
time.Sleep(time.Duration(attempt) * time.Second)
}
}
return false, maxAttempts, wasBlocked
}
func truncateString(s string, maxLen int) string {
if len(s) <= maxLen {
return s
}
return s[:maxLen-3] + "..."
}

===== FILE: README.md =====
![Golang Web Crawler Banner with Spider](Golang-Web-Crawler-Banner.jpg)
A powerful Go-based web crawler with an interactive terminal wizard interface. Features intelligent Cloudflare bypass strategies, comprehensive statistics, and support for HTML, PDF, and DOCX content 
scanning.
![Go Version](https:
![License](https:
---
| Mode | Description |
| ------------------------ | -------------------------------------------------------------------------- |
| **ğŸ”— Find Link** | Search for specific URLs/links across HTML pages, PDFs, and Word documents |
| **ğŸ“ Find Word/Phrase** | Search for any text string across all supported content types |
| **ğŸ’” Broken Link Check** | Scan entire site for 404s, timeouts, and connection errors |
| **ğŸ–¼ï¸ Oversized Images** | Find images exceeding a specified file size threshold |
| **ğŸ“„ Page Capture** | Generate PDFs, screenshots, or CMYK files for every page on the site |
Crawl only a specific section of a website by including the path in your URL:
```
ğŸŒ What site do you want to check?
(Tip: Include a path like /newsroom/ to only crawl that section)
â†’ https:
ğŸŒ² Detected path: /newsroom/news-releases/
ğŸ“ Only crawl pages under this path? (Y/n): y
âœ“ Will only crawl pages under /newsroom/news-releases/
```
This is useful for:
- Crawling only a blog, newsroom, or documentation section
- Avoiding irrelevant pages on large sites
- Faster, more focused crawls
**Smart Archive Detection:** For news/press release sections, the crawler automatically generates year/month archive URLs (e.g., `/newsroom/news-releases/2025/january/`) to discover all articles even 
when the listing page uses JavaScript pagination.
| Format | Output | Requirements |
| --------------------- | --------------- | -------------------- |
| **PDF only** | `.pdf` | Chrome/Chromium |
| **Images only** | `.png` | Chrome/Chromium |
| **Both PDF + Images** | `.pdf` + `.png` | Chrome/Chromium |
| **CMYK PDF** | `_cmyk.pdf` | Chrome + Ghostscript |
| **CMYK TIFF** | `_cmyk.tiff` | Chrome + ImageMagick |
The crawler employs multiple techniques to handle bot protection:
- **Alternative Entry Points**: Automatically tests 17+ common pages (`/about`, `/contact`, `/sitemap.xml`, etc.) when the main page is blocked
- **Custom Entry Point**: Specify your own "back door" URL
- **Multi-Phase Crawling**: Start from working pages, then retry blocked pages with established session cookies
- **User Agent Rotation**: Cycles through 5 different browser signatures
- **Session Persistence**: Maintains cookies across requests
- **Exponential Backoff**: Smart retry delays to avoid rate limiting
Real-time and final statistics include:
- Pages checked, matches found, errors, blocked pages
- Content breakdown (HTML, PDF, DOCX, images, links)
- Network stats (bytes downloaded, retries, blocked count)
- HTTP status code distribution (2xx, 3xx, 4xx, 5xx)
- Connection error categorization (timeouts, DNS, SSL, refused)
- Performance metrics (pages/second, avg download speed, avg page size)
- Cloudflare bypass stats (retried, recovered, still blocked, recovery rate)
---
- Go 1.21 or higher
- `pdfcpu` CLI tool (for PDF text extraction)
- Chrome or Chromium (for page capture mode)
- Ghostscript (optional, for CMYK PDF output)
- ImageMagick (optional, for CMYK TIFF output)
```bash
git clone <repository-url>
cd webcrawler
go mod tidy
go install github.com/pdfcpu/pdfcpu/cmd/pdfcpu@latest
export PATH=$PATH:$(go env GOPATH)/bin
sudo apt install chromium-browser
wget https:
sudo dpkg -i google-chrome-stable_current_amd64.deb
sudo apt install ghostscript
sudo apt install imagemagick
```
```bash
go run main.go
```
The interactive wizard will guide you through the configuration.
---
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•â•—
â•‘ ğŸ•·ï¸ Web Crawler Wizard ğŸ•·ï¸ â•‘
â•‘ v2.1 - Cloudflare Buster â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•â•
ğŸŒ What site do you want to check?
(Tip: Include a path like /newsroom/ to only crawl that section)
â†’ example.com
ğŸ” Testing connection to https:
ğŸ”„ Attempt 1/3 âœ… OK (200) - 245ms latency
ğŸ“‹ What should I check the site for?
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ğŸ”— Find a link on site (HTML, Word, PDF) â”‚
â”‚ 2. ğŸ“ Find a word/phrase on site (HTML, Word, PDF) â”‚
â”‚ 3. ğŸ’” Search for broken links â”‚
â”‚ 4. ğŸ–¼ï¸ Search for oversized images â”‚
â”‚ 5. ğŸ“„ Generate PDF for every page (with screenshots) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Enter choice (1-5): 2
ğŸ“ Enter the word or phrase to search for:
â†’ privacy policy
âš¡ Max concurrent requests (default 5, max 20): 10
ğŸ”„ Max retries per page (default 3): 3
```
To crawl only a specific section of a site, include the path in the URL:
```
ğŸŒ What site do you want to check?
(Tip: Include a path like /newsroom/ to only crawl that section)
â†’ https:
ğŸŒ² Detected path: /newsroom/news-releases/
ğŸ“ Only crawl pages under this path? (Y/n): y
âœ“ Will only crawl pages under /newsroom/news-releases/
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LAUNCH CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŒ Target: https:
â”‚ ğŸŒ² Path filter: /newsroom/news-releases/ â”‚
â”‚ ğŸ“Š Mode: Page Capture â”‚
â”‚ âš¡ Concurrency: 20 â”‚
â”‚ ğŸ”„ Max retries: 3 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
The crawler will only visit pages whose URL path starts with `/newsroom/news-releases/`, skipping all other sections of the site.
When you select option 5, you'll see a sub-menu for output format:
```
ğŸ“„ What format do you want to capture?
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ a. ğŸ“‘ PDF only â”‚
â”‚ b. ğŸ–¼ï¸ Images only (PNG) â”‚
â”‚ c. ğŸ“‘ğŸ–¼ï¸ Both PDF + Images â”‚
â”‚ d. ğŸ¨ CMYK PDF (for print) * â”‚
â”‚ e. ğŸ¨ CMYK TIFF (for InDesign) * â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
* Requires Ghostscript (d) or ImageMagick (e) installed
Enter choice (a/b/c/d/e): c
ğŸ“‘ğŸ–¼ï¸ Will generate both PDFs and PNG screenshots
ğŸ“ Output folder: ./page_captures/
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PAGE CAPTURE STARTING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¯ Target: https:
â”‚ ğŸŒ² Path: /newsroom/ â”‚
â”‚ ğŸ“ Output: page_captures_2024-01-15_14-30-00 â”‚
â”‚ ğŸ“‹ Format: PDF + Images â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ’¡ Press 'c' + Enter to cancel and save current progress â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Tip:** Press `c` + Enter at any time to stop crawling and keep the files captured so far.
When Cloudflare blocks the main page:
```
ğŸ” Testing connection to https:
ğŸ”„ Attempt 1/3 ğŸ›¡ï¸ CLOUDFLARE DETECTED (403)
â³ Waiting 3s before retry with different headers...
ğŸ”„ Attempt 2/3 ğŸ›¡ï¸ CLOUDFLARE DETECTED (403)
â³ Waiting 6s before retry with different headers...
ğŸ”„ Attempt 3/3 ğŸ›¡ï¸ CLOUDFLARE DETECTED (403)
ğŸ›¡ï¸ Cloudflare/Bot protection detected on main page!
ğŸ’¡ Let's try some alternative entry points...
Testing common entry points...
[ 1/17] Testing /about âœ… WORKS!
[ 2/17] Testing /about-us âŒ Failed
[ 3/17] Testing /contact âœ… WORKS!
[ 4/17] Testing /contact-us ğŸ›¡ï¸ Blocked
...
âœ… Found 2 working entry point(s)!
ğŸ”„ Will start from these and retry blocked pages later
```
---
During crawling, you'll see real-time updates with a progress bar:
```
ğŸ“Š [2m 15s] Pages: 142 | Matches: 8 | Errors: 3 | Blocked: 2 (Queue: 1, Recovered: 1) | 1.1 p/s | 45.2 KB/s
```
For page capture mode, you'll see a live progress bar with the current page being processed:
```
â ¹ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 60% â”‚ â± 4m 30s â”‚ ğŸ“‘ 45 captured â”‚ â³ 30 pending â”‚ âŒ 2 â”‚ 0.8/s
â†’ .../newsroom/news-releases/2025/december/name-1072620-en.html
```
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•â•—
â•‘ ğŸ“Š FINAL STATISTICS ğŸ“Š â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•â•£
â•‘ â•‘
â•‘ â±ï¸ Total Time: 5m 32s â•‘
â•‘ ğŸ“„ Pages Checked: 347 â•‘
â•‘ âœ… Matches Found: 23 â•‘
â•‘ ğŸ“ Results File: results-search-2024-01-15_14-30-00.csv â•‘
â•‘ â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•â•£
â•‘ ğŸ”¬ CONTENT BREAKDOWN â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•â•£
â•‘ ğŸ“ HTML Pages: 312 â•‘
â•‘ ğŸ“• PDF Documents: 28 â•‘
â•‘ ğŸ“˜ Word Documents: 7 â•‘
â•‘ ğŸ–¼ï¸ Images Checked: 0 â•‘
â•‘ ğŸ”— Links Checked: 0 â•‘
â•‘ â­ï¸ Skipped (External): 156 â•‘
...
```
For page capture mode:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•â•—
â•‘ ğŸ“Š PAGE CAPTURE COMPLETE ğŸ“Š â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•â•£
â•‘ â•‘
â•‘ â±ï¸ Total Time: 4m 30s â•‘
â•‘ ğŸ“„ Pages Visited: 180 â•‘
â•‘ ğŸ“‘ PDFs Generated: 152 â•‘
â•‘ ğŸ–¼ï¸ Images Generated: 152 â•‘
â•‘ âŒ Errors: 9 â•‘
â•‘ ğŸ“ Output Directory: page_captures_2024-01-15_14-30-00 â•‘
â•‘ â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•â•
```
Results are saved to timestamped CSV files:
**Search Mode:**
```csv
URL,ContentType,FoundIn,Target,Timestamp
https:
https:
```
**Broken Links Mode:**
```csv
BrokenURL,FoundOnPage,StatusCode,Error,Timestamp
https:
```
**Oversized Images Mode:**
```csv
ImageURL,FoundOnPage,SizeKB,ContentType,Timestamp
https:
```
**Page Capture Mode:**
Files are saved directly to a timestamped folder (e.g., `pdf_captures_2024-01-15_14-30-00/`):
- `index.pdf` / `index.png` - Homepage
- `about.pdf` / `about.png` - About page
- `contact_us.pdf` / `contact_us.png` - Contact page
- etc.
---
| Option | Default | Description |
| -------------------- | ------- | -------------------------------------------------------- |
| Concurrency | 5 | Number of concurrent requests (max 20) |
| Max Retries | 3 | Retry attempts per page on failure |
| Retry Delay | 2s | Base delay between retries (increases exponentially) |
| Blocked Retry Passes | 3 | Number of passes to retry blocked pages |
| Image Size Threshold | 500KB | Threshold for oversized image detection |
| Path Filter | (none) | Only crawl URLs starting with this path (e.g., `/blog/`) |
| Page Timeout | 180s | Max time to wait for a page to render (Page Capture) |
---
| Icon | Error Type | Description |
| ---- | ------------------ | ------------------------------------ |
| â±ï¸ | Timeout | Server not responding |
| ğŸš« | Connection Refused | Server actively refusing connections |
| ğŸŒ | DNS Error | Domain not found |
| ğŸ”’ | SSL/TLS Error | Certificate validation failed |
| Code | Handling |
| ------- | ------------------------------------------ |
| 200-299 | Success - content processed |
| 300-399 | Redirects followed (up to 10) |
| 403/503 | Bot protection detected - queued for retry |
| 429 | Rate limited - backed off and retried |
| 404 | Not found - logged as error |
| 5xx | Server error - retried |
Automatically identifies:
- Cloudflare ("Checking your browser...", "Ray ID")
- Incapsula/Imperva
- PerimeterX
- Sucuri
- Generic CAPTCHA challenges
- DDoS protection pages
---
```
webcrawler/
â”œâ”€â”€ main.go
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â”œâ”€â”€ assets/
â”‚ â””â”€â”€ tmp/
â””â”€â”€ internal/
â”œâ”€â”€ crawler/
â”‚ â”œâ”€â”€ crawler.go
â”‚ â””â”€â”€ pdfcapture.go
â””â”€â”€ parser/
â”œâ”€â”€ docx.go
â””â”€â”€ pdf.go
```
---
```bash
go install github.com/pdfcpu/pdfcpu/cmd/pdfcpu@latest
export PATH=$PATH:$(go env GOPATH)/bin
```
```bash
sudo apt install chromium-browser
wget https:
sudo dpkg -i google-chrome-stable_current_amd64.deb
sudo apt --fix-broken install -y
```
```bash
sudo apt install ghostscript
```
```bash
sudo apt install imagemagick
```
This means a page took longer than 180 seconds to render. Options:
- Ignore it (a few timeouts are normal for slow pages)
- Reduce concurrency to give Chrome more resources
- Some pages with heavy JavaScript may always timeout
If the crawler isn't finding all pages in a section:
- The site may use JavaScript-loaded content that doesn't expose links in the DOM
- For news/press release sections, the crawler auto-generates year/month archive URLs
- Try entering a more specific path or a known archive URL directly
- Some sites use infinite scroll or AJAX pagination that can't be fully crawled
This happens when the page body is loaded via JavaScript/AJAX after the initial page load:
- The crawler now waits for body content to stabilize before capturing
- If still seeing empty bodies, the site may use a complex loading pattern
- Try reducing concurrency to give Chrome more time to render
- Some heavily JavaScript-dependent pages may not capture well
- Reduce concurrency: `âš¡ Max concurrent requests: 3`
- Increase retry count
- Try running at a different time
- Some sites genuinely require JavaScript execution
The crawler automatically backs off, but you can:
- Reduce concurrency
- Increase the built-in delay (edit `time.Sleep(50 * time.Millisecond)` in `crawler.go`)
The crawler skips certificate verification by default (`InsecureSkipVerify: true`). This handles self-signed certs but be aware of the security implications.
---
```bash
go build -o webcrawler main.go
GOOS=linux GOARCH=amd64 go build -o webcrawler-linux main.go
GOOS=windows GOARCH=amd64 go build -o webcrawler.exe main.go
GOOS=darwin GOARCH=amd64 go build -o webcrawler-mac main.go
```
---
- [golang.org/x/net](https:
- [baliance.com/gooxml](https:
- [pdfcpu](https:
- [chromedp](https:
---
- Always respect `robots.txt` (manual check recommended)
- Be mindful of rate limits and server load
- Only crawl sites you have permission to access
- This tool is for legitimate purposes like SEO auditing, content verification, and site maintenance
---
MIT License - feel free to use, modify, and distribute.
---
Contributions welcome! Please feel free to submit issues and pull requests.
---
**Made with â¤ï¸ and Go**

===== FILE: roll_repo_for_ai.sh =====
set -uo pipefail
export TERM="${TERM:-xterm}"
safe_clear() { clear 2>/dev/null || true; }
safe_tput() { tput "$@" 2>/dev/null || true; }
REPO_DIR="${1:-.}"
MAX_SIZE_KB="${2:-40}"
MAX_SIZE_BYTES=$((MAX_SIZE_KB * 1024))
OUT_DIR="rolled_repo"
RED='\033[0;31m'; GREEN='\033[0;32m'; YELLOW='\033[1;33m'
BLUE='\033[0;34m'; CYAN='\033[0;36m'; WHITE='\033[1;37m'
DIM='\033[2m'; NC='\033[0m'; BOLD='\033[1m'
declare -A SELECTED
declare -a TREE_ITEMS TREE_PATHS TREE_DEPTHS TREE_TYPES
CURSOR=0
cd "$REPO_DIR"
mkdir -p "$OUT_DIR"
get_files() {
git ls-files --cached --others --exclude-standard | \
grep -vE '(\.lock$|bun\.lockb|package-lock\.json|yarn\.lock|pnpm-lock\.yaml)' | \
grep -vE '(^\.env$|\.env\..*)' | \
grep -vE '(^\.git/|\.next/|\.cache/)' | \
grep -vE '(node_modules/|dist/|build/|out/|coverage/|public/)' | \
grep -vE '(^|/)(target|vendor|zig-(cache|out))(/|$)' | \
grep -vE '\.(exe|dll|so|dylib|a|o|obj|lib|pdb|ilk|exp|wasm|elf)(\..+)?$' | \
sort
}
build_tree() {
local files="$1"
declare -A dirs_added
TREE_ITEMS=(); TREE_PATHS=(); TREE_DEPTHS=(); TREE_TYPES=()
while IFS= read -r file; do
[[ -z "$file" ]] && continue
local dir_path=""
IFS='/' read -ra parts <<< "$file"
local depth=0
for ((i=0; i<${
dir_path="${dir_path:+$dir_path/}${parts[i]}"
if [[ -z "${dirs_added[$dir_path]:-}" ]]; then
dirs_added[$dir_path]=1
TREE_ITEMS+=("${parts[i]}")
TREE_PATHS+=("$dir_path")
TREE_DEPTHS+=($depth)
TREE_TYPES+=("dir")
SELECTED["$dir_path"]=1
fi
((depth++))
done
TREE_ITEMS+=("${parts[-1]}")
TREE_PATHS+=("$file")
TREE_DEPTHS+=($depth)
TREE_TYPES+=("file")
SELECTED["$file"]=1
done <<< "$files"
}
toggle_selection() {
local idx=$1
local path="${TREE_PATHS[$idx]}"
local type="${TREE_TYPES[$idx]}"
if [[ "${SELECTED[$path]:-0}" == "1" ]]; then
SELECTED["$path"]=0
if [[ "$type" == "dir" ]]; then
for ((i=0; i<${
[[ "${TREE_PATHS[$i]}" == "$path/"* ]] && SELECTED["${TREE_PATHS[$i]}"]=0
done
fi
else
SELECTED["$path"]=1
if [[ "$type" == "dir" ]]; then
for ((i=0; i<${
[[ "${TREE_PATHS[$i]}" == "$path/"* ]] && SELECTED["${TREE_PATHS[$i]}"]=1
done
fi
local parent_path="$path"

