
===== FILE: go.mod =====
module webcrawler
go 1.24
toolchain go1.24.5
require (
baliance.com/gooxml v1.0.1
github.com/chromedp/cdproto v0.0.0-20250803210736-d308e07a266d
github.com/chromedp/chromedp v0.14.2
golang.org/x/net v0.19.0
)
require (
github.com/chromedp/sysutil v1.1.0
github.com/go-json-experiment/json v0.0.0-20250725192818-e39067aee2d2
github.com/gobwas/httphead v0.1.0
github.com/gobwas/pool v0.2.1
github.com/gobwas/ws v1.4.0
golang.org/x/sys v0.34.0
)

===== FILE: go.sum =====
baliance.com/gooxml v1.0.1 h1:fG5lmxmjEVFfbKQ2NuyCuU3hMuuOb5avh5a38SZNO1o=
baliance.com/gooxml v1.0.1/go.mod h1:+gpUgmkAF4zCtwOFPNRLDAvpVRWoKs5EeQTSv/HYFnw=
github.com/chromedp/cdproto v0.0.0-20250803210736-d308e07a266d h1:ZtA1sedVbEW7EW80Iz2GR3Ye6PwbJAJXjv7D74xG6HU=
github.com/chromedp/cdproto v0.0.0-20250803210736-d308e07a266d/go.mod h1:NItd7aLkcfOA/dcMXvl8p1u+lQqioRMq/SqDp71Pb/k=
github.com/chromedp/chromedp v0.14.2 h1:r3b/WtwM50RsBZHMUm9fsNhhzRStTHrKdr2zmwbZSzM=
github.com/chromedp/chromedp v0.14.2/go.mod h1:rHzAv60xDE7VNy/MYtTUrYreSc0ujt2O1/C3bzctYBo=
github.com/chromedp/sysutil v1.1.0 h1:PUFNv5EcprjqXZD9nJb9b/c9ibAbxiYo4exNWZyipwM=
github.com/chromedp/sysutil v1.1.0/go.mod h1:WiThHUdltqCNKGc4gaU50XgYjwjYIhKWoHGPTUfWTJ8=
github.com/go-json-experiment/json v0.0.0-20250725192818-e39067aee2d2 h1:iizUGZ9pEquQS5jTGkh4AqeeHCMbfbjeb0zMt0aEFzs=
github.com/go-json-experiment/json v0.0.0-20250725192818-e39067aee2d2/go.mod h1:TiCD2a1pcmjd7YnhGH0f/zKNcCD06B029pHhzV23c2M=
github.com/gobwas/httphead v0.1.0 h1:exrUm0f4YX0L7EBwZHuCF4GDp8aJfVeBrlLQrs6NqWU=
github.com/gobwas/httphead v0.1.0/go.mod h1:O/RXo79gxV8G+RqlR/otEwx4Q36zl9rqC5u12GKvMCM=
github.com/gobwas/pool v0.2.1 h1:xfeeEhW7pwmX8nuLVlqbzVc7udMDrwetjEv+TZIz1og=
github.com/gobwas/pool v0.2.1/go.mod h1:q8bcK0KcYlCgd9e7WYLm9LpyS+YeLd8JVDW6WezmKEw=
github.com/gobwas/ws v1.4.0 h1:CTaoG1tojrh4ucGPcoJFiAQUAsEWekEWvLy7GsVNqGs=
github.com/gobwas/ws v1.4.0/go.mod h1:G3gNqMNtPppf5XUz7O4shetPpcZ1VJ7zt18dlUeakrc=
github.com/ledongthuc/pdf v0.0.0-20220302134840-0c2507a12d80 h1:6Yzfa6GP0rIo/kULo2bwGEkFvCePZ3qHDDTC3/J9Swo=
github.com/ledongthuc/pdf v0.0.0-20220302134840-0c2507a12d80/go.mod h1:imJHygn/1yfhB7XSJJKlFZKl/J+dCPAknuiaGOshXAs=
github.com/orisano/pixelmatch v0.0.0-20220722002657-fb0b55479cde h1:x0TT0RDC7UhAVbbWWBzr41ElhJx5tXPWkIHA2HWPRuw=
github.com/orisano/pixelmatch v0.0.0-20220722002657-fb0b55479cde/go.mod h1:nZgzbfBr3hhjoZnS66nKrHmduYNpc34ny7RK4z5/HM0=
golang.org/x/net v0.19.0 h1:zTwKpTd2XuCqf8huc7Fo2iSy+4RHPd10s4KzeTnVr1c=
golang.org/x/net v0.19.0/go.mod h1:CfAk/cbD4CthTvqiEl8NpboMuiuOYsAr/7NOjZJtv1U=
golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.34.0 h1:H5Y5sJ2L2JRdyv7ROF1he/lPdvFsd0mJHFw2ThKHxLA=
golang.org/x/sys v0.34.0/go.mod h1:BJP2sWEmIv4KK5OTEluFJCKSidICx8ciO85XgH3Ak8k=

===== FILE: internal/crawler/crawler.go =====
package crawler
import (
"bytes"
"compress/gzip"
"crypto/tls"
"encoding/csv"
"fmt"
"io"
"net/http"
"net/http/cookiejar"
"net/url"
"os"
"strconv"
"strings"
"sync"
"sync/atomic"
"time"
"webcrawler/internal/parser"
"golang.org/x/net/html"
)
type SearchMode int
const (
ModeSearchLink SearchMode = iota + 1
ModeSearchWord
ModeBrokenLinks
ModeOversizedImages
ModePDFCapture
)
func (m SearchMode) String() string {
switch m {
case ModeSearchLink:
return "Find Link"
case ModeSearchWord:
return "Find Word/Phrase"
case ModeBrokenLinks:
return "Broken Link Check"
case ModeOversizedImages:
return "Oversized Image Check"
case ModePDFCapture:
return "Page Capture"
default:
return "Unknown"
}
}
type CaptureFormat int
const (
CapturePDFOnly CaptureFormat = iota + 1
CaptureImagesOnly
CaptureBoth
CaptureCMYKPDF
CaptureCMYKTIFF
)
func (c CaptureFormat) String() string {
switch c {
case CapturePDFOnly:
return "PDF only"
case CaptureImagesOnly:
return "Images only (PNG)"
case CaptureBoth:
return "PDF + Images"
case CaptureCMYKPDF:
return "CMYK PDF (for print)"
case CaptureCMYKTIFF:
return "CMYK TIFF (for InDesign)"
default:
return "Unknown"
}
}
type Config struct {
StartURL string
AltEntryPoints []string
Mode SearchMode
SearchTarget string
MaxConcurrency int
ImageSizeThreshold int64
MaxRetries int
RetryDelay time.Duration
RetryBlockedPages bool
BlockedRetryPasses int
CaptureFormat CaptureFormat
PathFilter string
}
type Stats struct {
PagesChecked int64
PagesQueued int64
MatchesFound int64
ErrorCount int64
BlockedCount int64
RetryCount int64
BytesDownloaded int64
PDFsScanned int64
DOCXScanned int64
HTMLScanned int64
ImagesChecked int64
LinksChecked int64
SkippedExternal int64
Status2xx int64
Status3xx int64
Status4xx int64
Status5xx int64
Timeouts int64
DNSErrors int64
SSLErrors int64
ConnectionRefused int64
BlockedRetried int64
BlockedRecovered int64
}
type BlockedPage struct {
URL string
Attempts int
LastError string
}
var (
visited sync.Map
blockedQueue sync.Map
wg sync.WaitGroup
sema chan struct{}
csvMu sync.Mutex
stats Stats
startTime time.Time
httpClient *http.Client
resultFile string
config Config
baseURL *url.URL
successfulHit bool
successMu sync.Mutex
)
var userAgents = []string{
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
}
func init() {
jar, _ := cookiejar.New(nil)
httpClient = &http.Client{
Timeout: 30 * time.Second,
Jar: jar,
Transport: &http.Transport{
TLSClientConfig: &tls.Config{InsecureSkipVerify: true},
DisableKeepAlives: false,
MaxIdleConns: 100,
MaxIdleConnsPerHost: 10,
IdleConnTimeout: 90 * time.Second,
},
CheckRedirect: func(req *http.Request, via []*http.Request) error {
if len(via) >= 10 {
return fmt.Errorf("stopped after 10 redirects")
}
if len(via) > 0 {
for key, val := range via[0].Header {
req.Header[key] = val
}
}
return nil
},
}
}
func Start(cfg Config) {
visited = sync.Map{}
blockedQueue = sync.Map{}
stats = Stats{}
startTime = time.Now()
config = cfg
successfulHit = false
sema = make(chan struct{}, cfg.MaxConcurrency)
var err error
baseURL, err = url.Parse(cfg.StartURL)
if err != nil {
fmt.Printf("âŒ Invalid start URL: %v\n", err)
return
}
timestamp := time.Now().Format("2006-01-02_15-04-05")
switch cfg.Mode {
case ModeSearchLink, ModeSearchWord:
resultFile = fmt.Sprintf("results-search-%s.csv", timestamp)
case ModeBrokenLinks:
resultFile = fmt.Sprintf("results-broken-links-%s.csv", timestamp)
case ModeOversizedImages:
resultFile = fmt.Sprintf("results-oversized-images-%s.csv", timestamp)
case ModePDFCapture:
StartPDFCapture(cfg)
return
}
createCSV()
stopStats := make(chan bool)
go printLiveStats(stopStats)
fmt.Println("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CRAWL STARTING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
fmt.Printf("â”‚ ğŸ¯ Target: %-40s â”‚\n", truncateString(cfg.StartURL, 40))
fmt.Println("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
fmt.Println()
if len(cfg.AltEntryPoints) > 0 {
fmt.Println("ğŸšª PHASE 1: Starting from alternative entry points...")
fmt.Println()
for i, entryPoint := range cfg.AltEntryPoints {
fmt.Printf(" ğŸ“ Entry point %d/%d: %s\n", i+1, len(cfg.AltEntryPoints), entryPoint)
crawl(entryPoint)
}
blockedQueue.Store(cfg.StartURL, &BlockedPage{URL: cfg.StartURL, Attempts: 0})
} else {
crawl(cfg.StartURL)
}
wg.Wait()
if cfg.RetryBlockedPages {
for pass := 1; pass <= cfg.BlockedRetryPasses; pass++ {
blockedCount := countBlockedQueue()
if blockedCount == 0 {
break
}
fmt.Printf("\n\nğŸ”„ PHASE %d: RETRYING BLOCKED PAGES (%d pages)\n", pass+1, blockedCount)
fmt.Println(" ğŸ’¡ Using cookies/session from successful requests...")
fmt.Println()
if pass > 1 {
delay := time.Duration(pass*5) * time.Second
fmt.Printf(" â³ Waiting %v before retry pass...\n", delay)
time.Sleep(delay)
}
retryBlockedPages()
wg.Wait()
}
}
stopStats <- true
printFinalStats()
}
func countBlockedQueue() int {
count := 0
blockedQueue.Range(func(key, value interface{}) bool {
count++
return true
})
return count
}
func retryBlockedPages() {
blockedQueue.Range(func(key, value interface{}) bool {
pageURL := key.(string)
page := value.(*BlockedPage)
if page.Attempts >= config.BlockedRetryPasses {
return true
}
page.Attempts++
atomic.AddInt64(&stats.BlockedRetried, 1)
blockedQueue.Delete(pageURL)
visited.Delete(pageURL)
wg.Add(1)
go func(link string, attemptNum int) {
defer wg.Done()
sema <- struct{}{}
defer func() { <-sema }()
fmt.Printf(" ğŸ”„ Retrying: %s\n", link)
time.Sleep(time.Duration(attemptNum) * time.Second)
success := fetchPageForRetry(link, attemptNum)
if success {
atomic.AddInt64(&stats.BlockedRecovered, 1)
fmt.Printf(" âœ… RECOVERED: %s\n", link)
}
}(pageURL, page.Attempts)
return true
})
}
func printLiveStats(stop chan bool) {
ticker := time.NewTicker(3 * time.Second)
defer ticker.Stop()
for {
select {
case <-stop:
return
case <-ticker.C:
elapsed := time.Since(startTime)
checked := atomic.LoadInt64(&stats.PagesChecked)
matches := atomic.LoadInt64(&stats.MatchesFound)
errors := atomic.LoadInt64(&stats.ErrorCount)
blocked := atomic.LoadInt64(&stats.BlockedCount)
bytesDown := atomic.LoadInt64(&stats.BytesDownloaded)
recovered := atomic.LoadInt64(&stats.BlockedRecovered)
pagesPerSec := float64(checked) / elapsed.Seconds()
bytesPerSec := float64(bytesDown) / elapsed.Seconds()
blockedQueueSize := countBlockedQueue()
fmt.Printf("\rğŸ“Š [%s] Pages: %d | Matches: %d | Errors: %d | Blocked: %d (Queue: %d, Recovered: %d) | %.1f p/s | %s/s ",
formatDuration(elapsed),
checked,
matches,
errors,
blocked,
blockedQueueSize,
recovered,
pagesPerSec,
formatBytes(int64(bytesPerSec)),
)
}
}
}
func printFinalStats() {
elapsed := time.Since(startTime)
fmt.Println()
fmt.Println()
fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•—")
fmt.Println("â•‘ ğŸ“Š FINAL STATISTICS ğŸ“Š â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Println("â•‘ â•‘")
fmt.Printf("â•‘ â±ï¸ Total Time: %-40s â•‘\n", formatDuration(elapsed))
fmt.Printf("â•‘ ğŸ“„ Pages Checked: %-40d â•‘\n", stats.PagesChecked)
fmt.Printf("â•‘ âœ… Matches Found: %-40d â•‘\n", stats.MatchesFound)
fmt.Printf("â•‘ ğŸ“ Results File: %-40s â•‘\n", truncateString(resultFile, 40))
fmt.Println("â•‘ â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Println("â•‘ ğŸ”¬ CONTENT BREAKDOWN â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Printf("â•‘ ğŸ“ HTML Pages: %-40d â•‘\n", stats.HTMLScanned)
fmt.Printf("â•‘ ğŸ“• PDF Documents: %-40d â•‘\n", stats.PDFsScanned)
fmt.Printf("â•‘ ğŸ“˜ Word Documents: %-40d â•‘\n", stats.DOCXScanned)
fmt.Printf("â•‘ ğŸ–¼ï¸ Images Checked: %-40d â•‘\n", stats.ImagesChecked)
fmt.Printf("â•‘ ğŸ”— Links Checked: %-40d â•‘\n", stats.LinksChecked)
fmt.Printf("â•‘ â­ï¸ Skipped (External): %-40d â•‘\n", stats.SkippedExternal)
fmt.Println("â•‘ â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Println("â•‘ ğŸ“¡ NETWORK STATS â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Printf("â•‘ ğŸ“¥ Data Downloaded: %-40s â•‘\n", formatBytes(stats.BytesDownloaded))
fmt.Printf("â•‘ ğŸ”„ Total Retries: %-40d â•‘\n", stats.RetryCount)
fmt.Printf("â•‘ âŒ Errors: %-40d â•‘\n", stats.ErrorCount)
fmt.Printf("â•‘ ğŸ›¡ï¸ Blocked (Bot Detect): %-40d â•‘\n", stats.BlockedCount)
fmt.Println("â•‘ â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Println("â•‘ ğŸšª CLOUDFLARE BYPASS STATS â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Printf("â•‘ ğŸ”„ Blocked Pages Retried: %-40d â•‘\n", stats.BlockedRetried)
fmt.Printf("â•‘ âœ… Successfully Recovered:%-40d â•‘\n", stats.BlockedRecovered)
blockedRemaining := countBlockedQueue()
fmt.Printf("â•‘ âŒ Still Blocked: %-40d â•‘\n", blockedRemaining)
if stats.BlockedRetried > 0 {
recoveryRate := float64(stats.BlockedRecovered) / float64(stats.BlockedRetried) * 100
fmt.Printf("â•‘ ğŸ“ˆ Recovery Rate: %-40s â•‘\n", fmt.Sprintf("%.1f%%", recoveryRate))
}
fmt.Println("â•‘ â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Println("â•‘ ğŸ“¶ HTTP STATUS CODES â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Printf("â•‘ âœ… 2xx (Success): %-40d â•‘\n", stats.Status2xx)
fmt.Printf("â•‘ â†ªï¸ 3xx (Redirect): %-40d â•‘\n", stats.Status3xx)
fmt.Printf("â•‘ âš ï¸ 4xx (Client Error): %-40d â•‘\n", stats.Status4xx)
fmt.Printf("â•‘ ğŸ”¥ 5xx (Server Error): %-40d â•‘\n", stats.Status5xx)
fmt.Println("â•‘ â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Println("â•‘ ğŸ”Œ CONNECTION ERRORS â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Printf("â•‘ â±ï¸ Timeouts: %-40d â•‘\n", stats.Timeouts)
fmt.Printf("â•‘ ğŸŒ DNS Errors: %-40d â•‘\n", stats.DNSErrors)
fmt.Printf("â•‘ ğŸ”’ SSL/TLS Errors: %-40d â•‘\n", stats.SSLErrors)
fmt.Printf("â•‘ ğŸš« Connection Refused: %-40d â•‘\n", stats.ConnectionRefused)
fmt.Println("â•‘ â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Println("â•‘ âš¡ PERFORMANCE â•‘")
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
pagesPerSec := float64(stats.PagesChecked) / elapsed.Seconds()
bytesPerSec := float64(stats.BytesDownloaded) / elapsed.Seconds()
avgPageSize := int64(0)
if stats.PagesChecked > 0 {
avgPageSize = stats.BytesDownloaded / stats.PagesChecked
}
fmt.Printf("â•‘ ğŸ“ˆ Pages/Second: %-40.2f â•‘\n", pagesPerSec)
fmt.Printf("â•‘ ğŸ“Š Avg Download Speed: %-40s â•‘\n", formatBytes(int64(bytesPerSec))+"/s")
fmt.Printf("â•‘ ğŸ“ Avg Page Size: %-40s â•‘\n", formatBytes(avgPageSize))
fmt.Println("â•‘ â•‘")
fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•")
if blockedRemaining > 0 {
fmt.Printf("\nâš ï¸ WARNING: %d pages still blocked after all retry attempts\n", blockedRemaining)
fmt.Println(" ğŸ’¡ Tips:")
fmt.Println(" - Try running again later (Cloudflare might be more lenient)")
fmt.Println(" - Reduce concurrency to look less like a bot")
fmt.Println(" - Some pages may genuinely require browser JavaScript")
}
if stats.BlockedRecovered > 0 {
fmt.Printf("\nâœ… SUCCESS: Recovered %d pages that were initially blocked!\n", stats.BlockedRecovered)
fmt.Println(" ğŸ’¡ The alternative entry point strategy worked!")
}
if stats.ErrorCount > 10 {
fmt.Printf("\nâš ï¸ WARNING: High error count (%d errors)\n", stats.ErrorCount)
fmt.Println(" ğŸ’¡ The site may be having issues or blocking requests")
}
}
func formatDuration(d time.Duration) string {
d = d.Round(time.Second)
h := d / time.Hour
d -= h * time.Hour
m := d / time.Minute
d -= m * time.Minute
s := d / time.Second
if h > 0 {
return fmt.Sprintf("%dh %dm %ds", h, m, s)
}
if m > 0 {
return fmt.Sprintf("%dm %ds", m, s)
}
return fmt.Sprintf("%ds", s)
}
func formatBytes(b int64) string {
const unit = 1024
if b < unit {
return fmt.Sprintf("%d B", b)
}
div, exp := int64(unit), 0
for n := b / unit; n >= unit; n /= unit {
div *= unit
exp++
}
return fmt.Sprintf("%.1f %cB", float64(b)/float64(div), "KMGTPE"[exp])
}
func truncateString(s string, maxLen int) string {
if len(s) <= maxLen {
return s
}
return s[:maxLen-3] + "..."
}
func createCSV() {
f, _ := os.Create(resultFile)
defer f.Close()
w := csv.NewWriter(f)
defer w.Flush()
switch config.Mode {
case ModeSearchLink, ModeSearchWord:
w.Write([]string{"URL", "ContentType", "FoundIn", "Target", "Timestamp"})
case ModeBrokenLinks:
w.Write([]string{"BrokenURL", "FoundOnPage", "StatusCode", "Error", "Timestamp"})
case ModeOversizedImages:
w.Write([]string{"ImageURL", "FoundOnPage", "SizeKB", "ContentType", "Timestamp"})
}
}
func writeSearchResult(pageURL, contentType, foundIn string) {
csvMu.Lock()
defer csvMu.Unlock()
atomic.AddInt64(&stats.MatchesFound, 1)
f, _ := os.OpenFile(resultFile, os.O_APPEND|os.O_WRONLY, 0644)
defer f.Close()
w := csv.NewWriter(f)
defer w.Flush()
w.Write([]string{pageURL, contentType, foundIn, config.SearchTarget, time.Now().Format(time.RFC3339)})
}
func writeBrokenLink(brokenURL, foundOnPage string, statusCode int, errMsg string) {
csvMu.Lock()
defer csvMu.Unlock()
atomic.AddInt64(&stats.MatchesFound, 1)
f, _ := os.OpenFile(resultFile, os.O_APPEND|os.O_WRONLY, 0644)
defer f.Close()
w := csv.NewWriter(f)
defer w.Flush()
w.Write([]string{brokenURL, foundOnPage, strconv.Itoa(statusCode), errMsg, time.Now().Format(time.RFC3339)})
}
func writeOversizedImage(imageURL, foundOnPage string, sizeKB int64, contentType string) {
csvMu.Lock()
defer csvMu.Unlock()
atomic.AddInt64(&stats.MatchesFound, 1)
f, _ := os.OpenFile(resultFile, os.O_APPEND|os.O_WRONLY, 0644)
defer f.Close()
w := csv.NewWriter(f)
defer w.Flush()
w.Write([]string{imageURL, foundOnPage, strconv.FormatInt(sizeKB, 10), contentType, time.Now().Format(time.RFC3339)})
}
func crawl(link string) {
if _, loaded := visited.LoadOrStore(link, true); loaded {
return
}
atomic.AddInt64(&stats.PagesQueued, 1)
wg.Add(1)
go func() {
defer wg.Done()
sema <- struct{}{}
defer func() { <-sema }()
fetchWithRetry(link)
}()
}
func fetchWithRetry(link string) {
var lastErr error
for attempt := 0; attempt <= config.MaxRetries; attempt++ {
if attempt > 0 {
atomic.AddInt64(&stats.RetryCount, 1)
delay := config.RetryDelay * time.Duration(attempt)
time.Sleep(delay)
}
success, blocked, err := fetchPage(link, attempt)
if success {
successMu.Lock()
successfulHit = true
successMu.Unlock()
return
}
if blocked {
blockedQueue.Store(link, &BlockedPage{URL: link, Attempts: 0, LastError: err.Error()})
return
}
lastErr = err
if err != nil {
errStr := err.Error()
if strings.Contains(errStr, "no such host") {
break
}
}
}
if lastErr != nil {
atomic.AddInt64(&stats.ErrorCount, 1)
}
}
func fetchPage(link string, attempt int) (success bool, blocked bool, err error) {
atomic.AddInt64(&stats.PagesChecked, 1)
req, err := http.NewRequest("GET", link, nil)
if err != nil {
return false, false, err
}
ua := userAgents[attempt%len(userAgents)]
req.Header.Set("User-Agent", ua)
req.Header.Set("Accept-Language", "en-US,en;q=0.5")
req.Header.Set("Accept-Encoding", "gzip, deflate, br")
req.Header.Set("DNT", "1")
req.Header.Set("Connection", "keep-alive")
req.Header.Set("Upgrade-Insecure-Requests", "1")
req.Header.Set("Referer", config.StartURL)
req.Header.Set("Sec-Fetch-Dest", "document")
req.Header.Set("Sec-Fetch-Mode", "navigate")
req.Header.Set("Sec-Fetch-Site", "same-origin")
resp, err := httpClient.Do(req)
if err != nil {
return false
}
defer resp.Body.Close()
if resp.StatusCode >= 400 {
if resp.StatusCode == 403 || resp.StatusCode == 503 || resp.StatusCode == 429 {
blockedQueue.Store(link, &BlockedPage{URL: link, Attempts: retryAttempt})
}
return false
}
contentType := resp.Header.Get("Content-Type")
var reader io.Reader = resp.Body
if resp.Header.Get("Content-Encoding") == "gzip" {
gzReader, err := gzip.NewReader(resp.Body)
if err != nil {
return false
}
defer gzReader.Close()
reader = gzReader
}
bodyBytes, err := io.ReadAll(reader)
if err != nil {
return false
}
atomic.AddInt64(&stats.BytesDownloaded, int64(len(bodyBytes)))
if detectBotProtection(string(bodyBytes)) {
blockedQueue.Store(link, &BlockedPage{URL: link, Attempts: retryAttempt})
return false
}
atomic.AddInt64(&stats.Status2xx, 1)
switch config.Mode {
case ModeSearchLink, ModeSearchWord:
processSearchMode(link, contentType, bodyBytes)
case ModeBrokenLinks:
if strings.Contains(contentType, "text/html") {
extractAndCheckLinks(bodyBytes, link)
}
case ModeOversizedImages:
if strings.Contains(contentType, "text/html") {
extractAndCheckImages(bodyBytes, link)
}
}
if strings.Contains(contentType, "text/html") {
atomic.AddInt64(&stats.HTMLScanned, 1)
extractInternalLinks(bodyBytes, link)
}
visited.Store(link, true)
return true
}
func processSearchMode(link, contentType string, bodyBytes []byte) {
target := config.SearchTarget
switch {
case strings.Contains(contentType, "application/pdf"):
atomic.AddInt64(&stats.PDFsScanned, 1)
if parser.ContainsLinkInPDF(bytes.NewReader(bodyBytes), target) {
fmt.Printf("\nâœ… MATCH FOUND IN PDF: %s\n", link)
writeSearchResult(link, contentType, "PDF")
}
case strings.Contains(contentType, "application/vnd.openxmlformats-officedocument.wordprocessingml.document"):
atomic.AddInt64(&stats.DOCXScanned, 1)
if parser.ContainsLinkInDocx(bytes.NewReader(bodyBytes), target) {
fmt.Printf("\nâœ… MATCH FOUND IN DOCX: %s\n", link)
writeSearchResult(link, contentType, "DOCX")
}
case strings.Contains(contentType, "text/html"):
if bytes.Contains(bodyBytes, []byte(target)) {
fmt.Printf("\nâœ… MATCH FOUND IN HTML: %s\n", link)
writeSearchResult(link, contentType, "HTML")
}
}
}
func extractAndCheckLinks(body []byte, pageURL string) {
doc, err := html.Parse(bytes.NewReader(body))
if err != nil {
return
}
var f func(*html.Node)
f = func(n *html.Node) {
if n.Type == html.ElementNode && n.Data == "a" {
for _, a := range n.Attr {
if a.Key == "href" && a.Val != "" &&
!strings.HasPrefix(a.Val, "
!strings.HasPrefix(a.Val, "mailto:") &&
!strings.HasPrefix(a.Val, "tel:") &&
!strings.HasPrefix(a.Val, "javascript:") {
checkLink(a.Val, pageURL)
}
}
}
for c := n.FirstChild; c != nil; c = c.NextSibling {
f(c)
}
}
f(doc)
}
func checkLink(href, pageURL string) {
u, err := url.Parse(href)
if err != nil || (u.Scheme != "" && u.Scheme != "http" && u.Scheme != "https") {
return
}
resolved := baseURL.ResolveReference(u).String()
atomic.AddInt64(&stats.LinksChecked, 1)
req, err := http.NewRequest("HEAD", resolved, nil)
if err != nil {
return
}
req.Header.Set("User-Agent", userAgents[0])
client := &http.Client{Timeout: 10 * time.Second}
resp, err := client.Do(req)
if err != nil {
writeBrokenLink(resolved, pageURL, 0, err.Error())
fmt.Printf("\nğŸ’” BROKEN LINK (error): %s\n", resolved)
return
}
defer resp.Body.Close()
if resp.StatusCode >= 400 {
writeBrokenLink(resolved, pageURL, resp.StatusCode, http.StatusText(resp.StatusCode))
fmt.Printf("\nğŸ’” BROKEN LINK (%d): %s\n", resp.StatusCode, resolved)
}
}
func extractAndCheckImages(body []byte, pageURL string) {
doc, err := html.Parse(bytes.NewReader(body))
if err != nil {
return
}
var f func(*html.Node)
f = func(n *html.Node) {
if n.Type == html.ElementNode && n.Data == "img" {
for _, a := range n.Attr {
if a.Key == "src" && a.Val != "" && !strings.HasPrefix(a.Val, "data:") {
checkImage(a.Val, pageURL)
}
}
}
for c := n.FirstChild; c != nil; c = c.NextSibling {
f(c)
}
}
f(doc)
}
func checkImage(src, pageURL string) {
u, err := url.Parse(src)
if err != nil {
return
}
resolved := baseURL.ResolveReference(u).String()
atomic.AddInt64(&stats.ImagesChecked, 1)
req, err := http.NewRequest("GET", resolved, nil)
if err != nil {
return
}
req.Header.Set("User-Agent", userAgents[0])
client := &http.Client{Timeout: 30 * time.Second}
resp, err := client.Do(req)
if err != nil {
return
}
defer resp.Body.Close()
if resp.StatusCode != 200 {
return
}
bodyBytes, err := io.ReadAll(resp.Body)
if err != nil {
return
}
sizeBytes := int64(len(bodyBytes))
sizeKB := sizeBytes / 1024
if sizeBytes > config.ImageSizeThreshold {
contentType := resp.Header.Get("Content-Type")
writeOversizedImage(resolved, pageURL, sizeKB, contentType)
fmt.Printf("\nğŸ–¼ï¸ OVERSIZED IMAGE (%dKB): %s\n", sizeKB, resolved)
}
}
func extractInternalLinks(body []byte, _ string) {
doc, err := html.Parse(bytes.NewReader(body))
if err != nil {
return
}
var f func(*html.Node)
f = func(n *html.Node) {
if n.Type == html.ElementNode && n.Data == "a" {
for _, a := range n.Attr {
if a.Key == "href" {
u, err := url.Parse(a.Val)
if err != nil || (u.Scheme != "" && u.Scheme != "http" && u.Scheme != "https") {
continue
}
next := baseURL.ResolveReference(u).String()
nextURL, err := url.Parse(next)
if err != nil {
continue
}
if nextURL.Host != baseURL.Host {
atomic.AddInt64(&stats.SkippedExternal, 1)
continue
}
time.Sleep(50 * time.Millisecond)
crawl(next)
}
}
}
for c := n.FirstChild; c != nil; c = c.NextSibling {
f(c)
}
}
f(doc)
}
func detectBotProtection(body string) bool {
indicators := []string{
"checking your browser",
"ddos protection",
"please enable javascript",
"access denied",
"security check",
"verify you are human",
"captcha",
"incapsula",
"perimeterx",
"sucuri",
"cloudflare",
"please wait while we verify",
"just a moment",
"ray id",
"attention required",
"sorry, you have been blocked",
}
bodyLower := strings.ToLower(body)
for _, indicator := range indicators {
if strings.Contains(bodyLower, indicator) {
return true
}
}
return false
}
func handleNetworkError(err error) {
errStr := err.Error()
switch {
case strings.Contains(errStr, "timeout"):
atomic.AddInt64(&stats.Timeouts, 1)
case strings.Contains(errStr, "connection refused"):
atomic.AddInt64(&stats.ConnectionRefused, 1)
case strings.Contains(errStr, "no such host"):
atomic.AddInt64(&stats.DNSErrors, 1)
case strings.Contains(errStr, "certificate"):
atomic.AddInt64(&stats.SSLErrors, 1)
}
}

===== FILE: internal/crawler/pdfcapture.go =====
package crawler
import (
"bufio"
"context"
"fmt"
"net/url"
"os"
"os/exec"
"path/filepath"
"regexp"
"strconv"
"strings"
"sync"
"sync/atomic"
"time"
"github.com/chromedp/cdproto/emulation"
"github.com/chromedp/cdproto/page"
"github.com/chromedp/chromedp"
)
type PDFCaptureStats struct {
PagesVisited int64
PagesQueued int64
PDFsGenerated int64
ScreenshotsGen int64
Errors int64
SkippedExternal int64
}
var (
cancelRequested int32
)
var (
pdfVisited sync.Map
pdfWg sync.WaitGroup
pdfSema chan struct{}
pdfStats PDFCaptureStats
pdfStartTime time.Time
pdfBaseURL *url.URL
pdfOutputDir string
pdfConcurrency int
pdfCaptureFormat CaptureFormat
pdfPathFilter string
pdfCurrentPage string
pdfCurrentMu sync.Mutex
)
func StartPDFCapture(cfg Config) {
pdfVisited = sync.Map{}
pdfStats = PDFCaptureStats{}
pdfStartTime = time.Now()
pdfConcurrency = cfg.MaxConcurrency
pdfCaptureFormat = cfg.CaptureFormat
pdfPathFilter = cfg.PathFilter
atomic.StoreInt32(&cancelRequested, 0)
if pdfCaptureFormat == 0 {
pdfCaptureFormat = CaptureBoth
}
var err error
pdfBaseURL, err = url.Parse(cfg.StartURL)
if err != nil {
fmt.Printf("âŒ Invalid start URL: %v\n", err)
return
}
timestamp := time.Now().Format("2006-01-02_15-04-05")
pdfOutputDir = fmt.Sprintf("page_captures_%s", timestamp)
os.MkdirAll(pdfOutputDir, 0755)
pdfSema = make(chan struct{}, cfg.MaxConcurrency)
stopStats := make(chan bool)
go printPDFLiveStats(stopStats)
stopKeyListener := make(chan bool)
go listenForCancel(stopKeyListener)
formatLabel := pdfCaptureFormat.String()
fmt.Println("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PAGE CAPTURE STARTING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
fmt.Printf("â”‚ ğŸ¯ Target: %-43s â”‚\n", truncateString(cfg.StartURL, 43))
if pdfPathFilter != "" {
fmt.Printf("â”‚ ğŸŒ² Path: %-43s â”‚\n", truncateString(pdfPathFilter, 43))
}
fmt.Printf("â”‚ ğŸ“ Output: %-43s â”‚\n", pdfOutputDir)
fmt.Printf("â”‚ ğŸ“‹ Format: %-43s â”‚\n", formatLabel)
fmt.Println("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
)
fmt.Println("â”‚ ğŸ’¡ Press 'c' + Enter to cancel and save current progress â”‚")
fmt.Println("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
)
fmt.Println()
crawlForPDF(cfg.StartURL)
pdfWg.Wait()
stopStats <- true
stopKeyListener <- true
printPDFFinalStats()
}
func listenForCancel(stop chan bool) {
reader := bufio.NewReader(os.Stdin)
inputChan := make(chan string)
go func() {
for {
input, err := reader.ReadString('\n')
if err != nil {
return
}
inputChan <- strings.TrimSpace(strings.ToLower(input))
}
}()
for {
select {
case <-stop:
return
case input := <-inputChan:
if input == "c" {
atomic.StoreInt32(&cancelRequested, 1)
fmt.Println("\n\nâ¹ï¸ CANCEL REQUESTED - Finishing current captures...")
fmt.Println(" (Waiting for in-progress pages to complete)")
return
}
}
}
}
func crawlForPDF(link string) {
if atomic.LoadInt32(&cancelRequested) == 1 {
return
}
link = normalizeURL(link)
if _, exists := pdfVisited.LoadOrStore(link, true); exists {
return
}
atomic.AddInt64(&pdfStats.PagesQueued, 1)
pdfWg.Add(1)
go func(pageURL string) {
defer pdfWg.Done()
pdfSema <- struct{}{}
defer func() { <-pdfSema }()
if atomic.LoadInt32(&cancelRequested) == 1 {
return
}
atomic.AddInt64(&pdfStats.PagesVisited, 1)
links := capturePage(pageURL)
if atomic.LoadInt32(&cancelRequested) == 0 {
for _, nextLink := range links {
crawlForPDF(nextLink)
}
}
}(link)
}
func capturePage(pageURL string) []string {
var extractedLinks []string
pdfCurrentMu.Lock()
pdfCurrentPage = pageURL
pdfCurrentMu.Unlock()
filename := sanitizeFilename(pageURL)
pdfPath := filepath.Join(pdfOutputDir, filename+".pdf")
pngPath := filepath.Join(pdfOutputDir, filename+".png")
switch pdfCaptureFormat {
case CapturePDFOnly:
if _, err := os.Stat(pdfPath); err == nil {
return nil
}
case CaptureImagesOnly:
if _, err := os.Stat(pngPath); err == nil {
return nil
}
case CaptureBoth:
if _, err := os.Stat(pdfPath); err == nil {
return nil
}
case CaptureCMYKPDF:
cmykPdfPath := filepath.Join(pdfOutputDir, filename+"_cmyk.pdf")
if _, err := os.Stat(cmykPdfPath); err == nil {
return nil
}
case CaptureCMYKTIFF:
tiffPath := filepath.Join(pdfOutputDir, filename+"_cmyk.tiff")
if _, err := os.Stat(tiffPath); err == nil {
return nil
}
}
opts := append(chromedp.DefaultExecAllocatorOptions[:],
chromedp.Flag("headless", true),
chromedp.Flag("disable-gpu", true),
chromedp.Flag("no-sandbox", true),
chromedp.Flag("disable-setuid-sandbox", true),
chromedp.Flag("disable-dev-shm-usage", true),
chromedp.Flag("disable-web-security", true),
chromedp.Flag("ignore-certificate-errors", true),
chromedp.WindowSize(1920, 1080),
)
allocCtx, allocCancel := chromedp.NewExecAllocator(context.Background(), opts...)
defer allocCancel()
ctx, cancel := chromedp.NewContext(allocCtx)
defer cancel()
ctx, cancel = context.WithTimeout(ctx, 180*time.Second)
defer cancel()
var pdfBuf []byte
var pngBuf []byte
var linksHTML string
actions := []chromedp.Action{
chromedp.Navigate(pageURL),
chromedp.WaitReady("body", chromedp.ByQuery),
chromedp.Sleep(2 * time.Second),
chromedp.Evaluate(`window.scrollTo(0, document.body.scrollHeight)`, nil),
chromedp.Sleep(1 * time.Second),
chromedp.Evaluate(`window.scrollTo(0, 0)`, nil),
chromedp.Sleep(500 * time.Millisecond),
chromedp.ActionFunc(func(ctx context.Context) error {
var lastLength int
stableCount := 0
for attempt := 0; attempt < 30; attempt++ {
var currentLength int
err := chromedp.Evaluate(`document.body.innerText.length`, &currentLength).Do(ctx)
if err != nil {
time.Sleep(500 * time.Millisecond)
continue
}
if currentLength > 500 && currentLength == lastLength {
stableCount++
if stableCount >= 3 {
return nil
}
} else {
stableCount = 0
}
lastLength = currentLength
time.Sleep(500 * time.Millisecond)
}
time.Sleep(2 * time.Second)
return nil
}),
chromedp.Sleep(1 * time.Second),
chromedp.Evaluate(`
Array.from(document.querySelectorAll('a[href]'))
.map(a => a.href)
.filter(href => href && !href.startsWith('javascript:') && !href.startsWith('mailto:') && !href.startsWith('tel:'))
.join('\n')
`, &linksHTML),
}
needsScreenshot := pdfCaptureFormat == CaptureImagesOnly ||
pdfCaptureFormat == CaptureBoth ||
pdfCaptureFormat == CaptureCMYKTIFF
if needsScreenshot {
actions = append(actions, chromedp.ActionFunc(func(ctx context.Context) error {
_, _, contentSize, _, _, _, err := page.GetLayoutMetrics().Do(ctx)
if err != nil {
return err
}
width, height := int64(contentSize.Width), int64(contentSize.Height)
if height > 16384 {
height = 16384
}
err = emulation.SetDeviceMetricsOverride(width, height, 1, false).
WithScreenOrientation(&emulation.ScreenOrientation{
Type: emulation.OrientationTypePortraitPrimary,
Angle: 0,
}).Do(ctx)
if err != nil {
return err
}
pngBuf, err = page.CaptureScreenshot().
WithQuality(100).
WithFromSurface(true).
Do(ctx)
return err
}))
}
needsPDF := pdfCaptureFormat == CapturePDFOnly ||
pdfCaptureFormat == CaptureBoth ||
pdfCaptureFormat == CaptureCMYKPDF
if needsPDF {
actions = append(actions, chromedp.ActionFunc(func(ctx context.Context) error {
var err error
pdfBuf, _, err = page.PrintToPDF().
WithPrintBackground(true).
WithScale(1.0).
WithPaperWidth(8.5).
WithPaperHeight(11).
WithMarginTop(0.4).
WithMarginBottom(0.4).
WithMarginLeft(0.4).
WithMarginRight(0.4).
WithDisplayHeaderFooter(false).
WithGenerateDocumentOutline(false).
Do(ctx)
return err
}))
}
err := chromedp.Run(ctx, actions...)
if err != nil {
atomic.AddInt64(&pdfStats.Errors, 1)
fmt.Print("\033[2K\r")
fmt.Printf("âŒ Error: %s - %v\n\n", truncateString(pageURL, 40), err)
return nil
}
if pdfCaptureFormat == CapturePDFOnly || pdfCaptureFormat == CaptureBoth {
if err := os.WriteFile(pdfPath, pdfBuf, 0644); err != nil {
atomic.AddInt64(&pdfStats.Errors, 1)
return extractedLinks
}
atomic.AddInt64(&pdfStats.PDFsGenerated, 1)
}
if pdfCaptureFormat == CaptureCMYKPDF {
tempPdfPath := filepath.Join(pdfOutputDir, filename+"_temp.pdf")
if err := os.WriteFile(tempPdfPath, pdfBuf, 0644); err != nil {
atomic.AddInt64(&pdfStats.Errors, 1)
return extractedLinks
}
cmykPdfPath := filepath.Join(pdfOutputDir, filename+"_cmyk.pdf")
if err := convertToCMYKPDF(tempPdfPath, cmykPdfPath); err != nil {
atomic.AddInt64(&pdfStats.Errors, 1)
os.Remove(tempPdfPath)
return extractedLinks
}
os.Remove(tempPdfPath)
atomic.AddInt64(&pdfStats.PDFsGenerated, 1)
}
if pdfCaptureFormat == CaptureImagesOnly || pdfCaptureFormat == CaptureBoth {
if err := os.WriteFile(pngPath, pngBuf, 0644); err != nil {
atomic.AddInt64(&pdfStats.Errors, 1)
return extractedLinks
}
atomic.AddInt64(&pdfStats.ScreenshotsGen, 1)
}
if pdfCaptureFormat == CaptureCMYKTIFF {
tempPngPath := filepath.Join(pdfOutputDir, filename+"_temp.png")
if err := os.WriteFile(tempPngPath, pngBuf, 0644); err != nil {
atomic.AddInt64(&pdfStats.Errors, 1)
return extractedLinks
}
tiffPath := filepath.Join(pdfOutputDir, filename+"_cmyk.tiff")
if err := convertToCMYKTIFF(tempPngPath, tiffPath); err != nil {
atomic.AddInt64(&pdfStats.Errors, 1)
os.Remove(tempPngPath)
return extractedLinks
}
os.Remove(tempPngPath)
atomic.AddInt64(&pdfStats.ScreenshotsGen, 1)
}
if linksHTML != "" {
for _, href := range strings.Split(linksHTML, "\n") {
href = strings.TrimSpace(href)
if href == "" {
continue
}
u, err := url.Parse(href)
if err != nil {
continue
}
if u.Host != pdfBaseURL.Host {
atomic.AddInt64(&pdfStats.SkippedExternal, 1)
continue
}
if pdfPathFilter != "" && !strings.HasPrefix(u.Path, pdfPathFilter) {
continue
}
extractedLinks = append(extractedLinks, href)
if q := u.Query(); q.Get("page") != "" {
if pageNum, err := strconv.Atoi(q.Get("page")); err == nil {
for i := 1; i <= 5; i++ {
newQ := u.Query()
newQ.Set("page", strconv.Itoa(pageNum+i))
newURL := *u
newURL.RawQuery = newQ.Encode()
extractedLinks = append(extractedLinks, newURL.String())
}
}
}
}
}
parsedPage, _ := url.Parse(pageURL)
if parsedPage != nil {
normalizedPath := strings.TrimSuffix(parsedPage.Path, "/")
normalizedFilter := strings.TrimSuffix(pdfPathFilter, "/")
isListingPage := strings.HasSuffix(parsedPage.Path, "/") ||
normalizedPath == normalizedFilter ||
parsedPage.Path == pdfPathFilter
if isListingPage {
if parsedPage.Query().Get("page") == "" {
for i := 2; i <= 10; i++ {
newQ := parsedPage.Query()
newQ.Set("page", strconv.Itoa(i))
newURL := *parsedPage
newURL.RawQuery = newQ.Encode()
extractedLinks = append(extractedLinks, newURL.String())
}
}
}
}
return extractedLinks
}
func sanitizeFilename(urlStr string) string {
u, err := url.Parse(urlStr)
if err != nil {
return "page"
}
name := u.Path
if name == "" || name == "/" {
name = "index"
}
name = strings.TrimPrefix(name, "/")
name = strings.ReplaceAll(name, "/", "_")
invalidChars := regexp.MustCompile(`[<>:"/\\|?*]`)
name = invalidChars.ReplaceAllString(name, "_")
if u.RawQuery != "" {
name += "_q" + hashString(u.RawQuery)[:8]
}
if len(name) > 200 {
name = name[:200]
}
name = strings.TrimRight(name, ". ")
if name == "" {
name = "page"
}
return name
}
func hashString(s string) string {
h := uint32(0)
for _, c := range s {
h = h*31 + uint32(c)
}
return fmt.Sprintf("%08x", h)
}
func normalizeURL(link string) string {
u, err := url.Parse(link)
if err != nil {
return link
}
u.Fragment = ""
if u.Path == "" {
u.Path = "/"
}
return u.String()
}
func printPDFLiveStats(stop chan bool) {
ticker := time.NewTicker(1 * time.Second)
defer ticker.Stop()
spinChars := []string{"â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "}
spinIdx := 0
for {
select {
case <-stop:
return
case <-ticker.C:
elapsed := time.Since(pdfStartTime)
queued := atomic.LoadInt64(&pdfStats.PagesQueued)
visited := atomic.LoadInt64(&pdfStats.PagesVisited)
pdfs := atomic.LoadInt64(&pdfStats.PDFsGenerated)
screenshots := atomic.LoadInt64(&pdfStats.ScreenshotsGen)
errors := atomic.LoadInt64(&pdfStats.Errors)
pdfCurrentMu.Lock()
currentPage := pdfCurrentPage
pdfCurrentMu.Unlock()
pagesPerSec := float64(visited) / elapsed.Seconds()
if elapsed.Seconds() < 1 {
pagesPerSec = 0
}
barWidth := 20
var pct int64 = 0
if queued > 0 {
pct = visited * 100 / queued
if pct > 100 {
pct = 100
}
}
filled := int(pct) * barWidth / 100
if filled > barWidth {
filled = barWidth
}
bar := ""
for i := 0; i < filled; i++ {
bar += "â–ˆ"
}
for i := filled; i < barWidth; i++ {
bar += "â–‘"
}
spinner := spinChars[spinIdx%len(spinChars)]
spinIdx++
displayURL := currentPage
if len(displayURL) > 60 {
displayURL = "..." + displayURL[len(displayURL)-57:]
}
fmt.Print("\033[2K\r")
pending := queued - visited
if pending < 0 {
pending = 0
}
switch pdfCaptureFormat {
case CapturePDFOnly:
fmt.Printf("%s \033[32m[%s]\033[0m %3d%% â”‚ â± %s â”‚ ğŸ“‘ %d captured â”‚ â³ %d pending â”‚ âŒ %d â”‚ %.1f/s\n",
spinner, bar, pct, formatDuration(elapsed), pdfs, pending, errors, pagesPerSec)
case CaptureImagesOnly:
fmt.Printf("%s \033[32m[%s]\033[0m %3d%% â”‚ â± %s â”‚ ğŸ–¼ï¸ %d captured â”‚ â³ %d pending â”‚ âŒ %d â”‚ %.1f/s\n",
spinner, bar, pct, formatDuration(elapsed), screenshots, pending, errors, pagesPerSec)
case CaptureBoth:
fmt.Printf("%s \033[32m[%s]\033[0m %3d%% â”‚ â± %s â”‚ ğŸ“‘ %d â”‚ ğŸ–¼ï¸ %d â”‚ â³ %d pending â”‚ âŒ %d â”‚ %.1f/s\n",
spinner, bar, pct, formatDuration(elapsed), pdfs, screenshots, pending, errors, pagesPerSec)
case CaptureCMYKPDF:
fmt.Printf("%s \033[32m[%s]\033[0m %3d%% â”‚ â± %s â”‚ ğŸ¨ %d CMYK â”‚ â³ %d pending â”‚ âŒ %d â”‚ %.1f/s\n",
spinner, bar, pct, formatDuration(elapsed), pdfs, pending, errors, pagesPerSec)
case CaptureCMYKTIFF:
fmt.Printf("%s \033[32m[%s]\033[0m %3d%% â”‚ â± %s â”‚ ğŸ¨ %d TIFF â”‚ â³ %d pending â”‚ âŒ %d â”‚ %.1f/s\n",
spinner, bar, pct, formatDuration(elapsed), screenshots, pending, errors, pagesPerSec)
}
fmt.Print("\033[2K")
if displayURL != "" {
fmt.Printf(" \033[2mâ†’ %s\033[0m", displayURL)
}
fmt.Print("\033[1A\r")
}
}
}
func printPDFFinalStats() {
elapsed := time.Since(pdfStartTime)
wasCancelled := atomic.LoadInt32(&cancelRequested) == 1
fmt.Print("\033[2K\r")
fmt.Println()
fmt.Println()
fmt.Println()
fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•—")
if wasCancelled {
fmt.Println("â•‘ ğŸ“Š PAGE CAPTURE CANCELLED ğŸ“Š â•‘")
} else {
fmt.Println("â•‘ ğŸ“Š PAGE CAPTURE COMPLETE ğŸ“Š â•‘")
}
fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•£")
fmt.Println("â•‘ â•‘")
fmt.Printf("â•‘ â±ï¸ Total Time: %-40s â•‘\n", formatDuration(elapsed))
fmt.Printf("â•‘ ğŸ“„ Pages Visited: %-40d â•‘\n", pdfStats.PagesVisited)
switch pdfCaptureFormat {
case CapturePDFOnly:
fmt.Printf("â•‘ ğŸ“‘ PDFs Generated: %-40d â•‘\n", pdfStats.PDFsGenerated)
case CaptureImagesOnly:
fmt.Printf("â•‘ ğŸ–¼ï¸ Images Generated: %-40d â•‘\n", pdfStats.ScreenshotsGen)
case CaptureBoth:
fmt.Printf("â•‘ ğŸ“‘ PDFs Generated: %-40d â•‘\n", pdfStats.PDFsGenerated)
fmt.Printf("â•‘ ğŸ–¼ï¸ Images Generated: %-40d â•‘\n", pdfStats.ScreenshotsGen)
case CaptureCMYKPDF:
fmt.Printf("â•‘ ğŸ¨ CMYK PDFs Generated: %-40d â•‘\n", pdfStats.PDFsGenerated)
case CaptureCMYKTIFF:
fmt.Printf("â•‘ ğŸ¨ CMYK TIFFs Generated: %-40d â•‘\n", pdfStats.ScreenshotsGen)
}
fmt.Printf("â•‘ âŒ Errors: %-40d â•‘\n", pdfStats.Errors)
fmt.Printf("â•‘ ğŸ“ Output Directory: %-40s â•‘\n", pdfOutputDir)
fmt.Println("â•‘ â•‘")
if wasCancelled {
fmt.Println("â•‘ â„¹ï¸ Crawl was cancelled early - partial results saved â•‘")
fmt.Println("â•‘ â•‘")
}
fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â
•â•â•â•â•â•â•")
}
func convertToCMYKPDF(inputPath, outputPath string) error {
if _, err := exec.LookPath("gs"); err != nil {
return fmt.Errorf("ghostscript (gs) not found in PATH - install with: sudo apt install ghostscript")
}
cmd := exec.Command("gs",
"-dSAFER",
"-dBATCH",
"-dNOPAUSE",
"-dNOCACHE",
"-sDEVICE=pdfwrite",
"-sColorConversionStrategy=CMYK",
"-dProcessColorModel=/DeviceCMYK",
"-dAutoRotatePages=/None",
"-sOutputFile="+outputPath,
inputPath,
)
output, err := cmd.CombinedOutput()
if err != nil {
return fmt.Errorf("ghostscript error: %v - %s", err, string(output))
}
return nil
}
func convertToCMYKTIFF(inputPath, outputPath string) error {
var cmdName string
if _, err := exec.LookPath("magick"); err == nil {
cmdName = "magick"
} else if _, err := exec.LookPath("convert"); err == nil {
cmdName = "convert"
} else {
return fmt.Errorf("imagemagick not found in PATH - install with: sudo apt install imagemagick")
}
args := []string{
inputPath,
"-colorspace", "CMYK",
"-compress", "LZW",
outputPath,
}
if cmdName == "magick" {
args = append([]string{"convert"}, args...)
}
cmd := exec.Command(cmdName, args...)
output, err := cmd.CombinedOutput()
if err != nil {
return fmt.Errorf("imagemagick error: %v - %s", err, string(output))
}
return nil
}

