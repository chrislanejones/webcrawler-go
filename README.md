# ğŸ•·ï¸ Web Crawler - Cloudflare Buster Edition

![Golang Web Crawler Banner with Spider](Golang-Web-Crawler-Banner.jpg)
A powerful Go-based web crawler with an interactive terminal wizard interface. Features intelligent Cloudflare bypass strategies, comprehensive statistics, and support for HTML, PDF, and DOCX content scanning.

![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=flat&logo=go)
![License](https://img.shields.io/badge/License-MIT-green.svg)

---

## âœ¨ Features

### ğŸ¯ Four Powerful Search Modes

| Mode                     | Description                                                                |
| ------------------------ | -------------------------------------------------------------------------- |
| **ğŸ”— Find Link**         | Search for specific URLs/links across HTML pages, PDFs, and Word documents |
| **ğŸ“ Find Word/Phrase**  | Search for any text string across all supported content types              |
| **ğŸ’” Broken Link Check** | Scan entire site for 404s, timeouts, and connection errors                 |
| **ğŸ–¼ï¸ Oversized Images**  | Find images exceeding a specified file size threshold                      |

### ğŸ›¡ï¸ Cloudflare Bypass Strategies

The crawler employs multiple techniques to handle bot protection:

- **Alternative Entry Points**: Automatically tests 17+ common pages (`/about`, `/contact`, `/sitemap.xml`, etc.) when the main page is blocked
- **Custom Entry Point**: Specify your own "back door" URL
- **Multi-Phase Crawling**: Start from working pages, then retry blocked pages with established session cookies
- **User Agent Rotation**: Cycles through 5 different browser signatures
- **Session Persistence**: Maintains cookies across requests
- **Exponential Backoff**: Smart retry delays to avoid rate limiting

### ğŸ“Š Comprehensive Statistics

Real-time and final statistics include:

- Pages checked, matches found, errors, blocked pages
- Content breakdown (HTML, PDF, DOCX, images, links)
- Network stats (bytes downloaded, retries, blocked count)
- HTTP status code distribution (2xx, 3xx, 4xx, 5xx)
- Connection error categorization (timeouts, DNS, SSL, refused)
- Performance metrics (pages/second, avg download speed, avg page size)
- Cloudflare bypass stats (retried, recovered, still blocked, recovery rate)

---

## ğŸš€ Quick Start

### Prerequisites

- Go 1.21 or higher
- `pdfcpu` CLI tool (for PDF text extraction)

### Installation

```bash
# Clone or download the project
git clone <repository-url>
cd webcrawler

# Install dependencies
go mod tidy

# Install pdfcpu for PDF support
go install github.com/pdfcpu/pdfcpu/cmd/pdfcpu@latest
export PATH=$PATH:$(go env GOPATH)/bin
```

### Running

```bash
go run main.go
```

The interactive wizard will guide you through the configuration.

---

## ğŸ“– Usage Guide

### Step-by-Step Wizard

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   ğŸ•·ï¸  Web Crawler Wizard  ğŸ•·ï¸                       â•‘
â•‘                        v2.1 - Cloudflare Buster                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŒ What site do you want to check?
   â†’ example.com

ğŸ” Testing connection to https://example.com...
   ğŸ”„ Attempt 1/3 âœ… OK (200) - 245ms latency

ğŸ“‹ What should I check the site for?

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  1. ğŸ”— Find a link on site (HTML, Word, PDF)            â”‚
   â”‚  2. ğŸ“ Find a word/phrase on site (HTML, Word, PDF)     â”‚
   â”‚  3. ğŸ’” Search for broken links                          â”‚
   â”‚  4. ğŸ–¼ï¸  Search for oversized images                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Enter choice (1-4): 2

ğŸ“ Enter the word or phrase to search for:
   â†’ privacy policy

âš¡ Max concurrent requests (default 5, max 20): 10

ğŸ”„ Max retries per page (default 3): 3
```

### Handling Cloudflare Protection

When Cloudflare blocks the main page:

```
ğŸ” Testing connection to https://protected-site.com...
   ğŸ”„ Attempt 1/3 ğŸ›¡ï¸  CLOUDFLARE DETECTED (403)
   â³ Waiting 3s before retry with different headers...
   ğŸ”„ Attempt 2/3 ğŸ›¡ï¸  CLOUDFLARE DETECTED (403)
   â³ Waiting 6s before retry with different headers...
   ğŸ”„ Attempt 3/3 ğŸ›¡ï¸  CLOUDFLARE DETECTED (403)

   ğŸ›¡ï¸  Cloudflare/Bot protection detected on main page!
   ğŸ’¡ Let's try some alternative entry points...

   Testing common entry points...

   [ 1/17] Testing /about                âœ… WORKS!
   [ 2/17] Testing /about-us             âŒ Failed
   [ 3/17] Testing /contact              âœ… WORKS!
   [ 4/17] Testing /contact-us           ğŸ›¡ï¸  Blocked
   ...

   âœ… Found 2 working entry point(s)!
   ğŸ”„ Will start from these and retry blocked pages later
```

---

## ğŸ“Š Output

### Live Statistics

During crawling, you'll see real-time updates:

```
ğŸ“Š [2m 15s] Pages: 142 | Matches: 8 | Errors: 3 | Blocked: 2 (Queue: 1, Recovered: 1) | 1.1 p/s | 45.2 KB/s
```

### Final Report

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      ğŸ“Š FINAL STATISTICS ğŸ“Š                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘  â±ï¸  Total Time:           5m 32s                                  â•‘
â•‘  ğŸ“„ Pages Checked:         347                                    â•‘
â•‘  âœ… Matches Found:         23                                     â•‘
â•‘  ğŸ“ Results File:          results-search-2024-01-15_14-30-00.csv â•‘
â•‘                                                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                      ğŸ”¬ CONTENT BREAKDOWN                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“ HTML Pages:            312                                    â•‘
â•‘  ğŸ“• PDF Documents:         28                                     â•‘
â•‘  ğŸ“˜ Word Documents:        7                                      â•‘
â•‘  ğŸ–¼ï¸  Images Checked:        0                                      â•‘
â•‘  ğŸ”— Links Checked:         0                                      â•‘
â•‘  â­ï¸  Skipped (External):    156                                    â•‘
...
```

### CSV Results

Results are saved to timestamped CSV files:

**Search Mode:**

```csv
URL,ContentType,FoundIn,Target,Timestamp
https://example.com/page1,text/html,HTML,privacy policy,2024-01-15T14:32:45Z
https://example.com/docs/terms.pdf,application/pdf,PDF,privacy policy,2024-01-15T14:33:12Z
```

**Broken Links Mode:**

```csv
BrokenURL,FoundOnPage,StatusCode,Error,Timestamp
https://example.com/old-page,https://example.com/links,404,Not Found,2024-01-15T14:32:45Z
```

**Oversized Images Mode:**

```csv
ImageURL,FoundOnPage,SizeKB,ContentType,Timestamp
https://example.com/hero.jpg,https://example.com/,2048,image/jpeg,2024-01-15T14:32:45Z
```

---

## âš™ï¸ Configuration Options

| Option               | Default | Description                                          |
| -------------------- | ------- | ---------------------------------------------------- |
| Concurrency          | 5       | Number of concurrent requests (max 20)               |
| Max Retries          | 3       | Retry attempts per page on failure                   |
| Retry Delay          | 2s      | Base delay between retries (increases exponentially) |
| Blocked Retry Passes | 3       | Number of passes to retry blocked pages              |
| Image Size Threshold | 500KB   | Threshold for oversized image detection              |

---

## ğŸš¨ Error Detection

### Network Errors

| Icon | Error Type         | Description                          |
| ---- | ------------------ | ------------------------------------ |
| â±ï¸   | Timeout            | Server not responding                |
| ğŸš«   | Connection Refused | Server actively refusing connections |
| ğŸŒ   | DNS Error          | Domain not found                     |
| ğŸ”’   | SSL/TLS Error      | Certificate validation failed        |

### HTTP Status Codes

| Code    | Handling                                   |
| ------- | ------------------------------------------ |
| 200-299 | Success - content processed                |
| 300-399 | Redirects followed (up to 10)              |
| 403/503 | Bot protection detected - queued for retry |
| 429     | Rate limited - backed off and retried      |
| 404     | Not found - logged as error                |
| 5xx     | Server error - retried                     |

### Bot Protection Detection

Automatically identifies:

- Cloudflare ("Checking your browser...", "Ray ID")
- Incapsula/Imperva
- PerimeterX
- Sucuri
- Generic CAPTCHA challenges
- DDoS protection pages

---

## ğŸ“ Project Structure

```
webcrawler/
â”œâ”€â”€ main.go                      # Interactive wizard & entry point
â”œâ”€â”€ go.mod                       # Go module definition
â”œâ”€â”€ go.sum                       # Dependency checksums
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ tmp/                     # Temporary files for PDF processing
â””â”€â”€ internal/
    â”œâ”€â”€ crawler/
    â”‚   â””â”€â”€ crawler.go           # Core crawling logic & statistics
    â””â”€â”€ parser/
        â”œâ”€â”€ docx.go              # Word document parser
        â””â”€â”€ pdf.go               # PDF text extractor
```

---

## ğŸ”§ Troubleshooting

### "pdfcpu: command not found"

```bash
go install github.com/pdfcpu/pdfcpu/cmd/pdfcpu@latest
export PATH=$PATH:$(go env GOPATH)/bin
```

### High blocked page count

- Reduce concurrency: `âš¡ Max concurrent requests: 3`
- Increase retry count
- Try running at a different time
- Some sites genuinely require JavaScript execution

### Rate limiting (429 errors)

The crawler automatically backs off, but you can:

- Reduce concurrency
- Increase the built-in delay (edit `time.Sleep(50 * time.Millisecond)` in `crawler.go`)

### SSL certificate errors

The crawler skips certificate verification by default (`InsecureSkipVerify: true`). This handles self-signed certs but be aware of the security implications.

---

## ğŸ› ï¸ Building

```bash
# Build for current platform
go build -o webcrawler main.go

# Cross-compile for Linux
GOOS=linux GOARCH=amd64 go build -o webcrawler-linux main.go

# Cross-compile for Windows
GOOS=windows GOARCH=amd64 go build -o webcrawler.exe main.go

# Cross-compile for macOS
GOOS=darwin GOARCH=amd64 go build -o webcrawler-mac main.go
```

---

## ğŸ“ Dependencies

- [golang.org/x/net](https://pkg.go.dev/golang.org/x/net) - HTML parsing
- [baliance.com/gooxml](https://github.com/baliance/gooxml) - DOCX parsing
- [pdfcpu](https://github.com/pdfcpu/pdfcpu) - PDF text extraction (external CLI)

---

## âš ï¸ Legal & Ethical Considerations

- Always respect `robots.txt` (manual check recommended)
- Be mindful of rate limits and server load
- Only crawl sites you have permission to access
- This tool is for legitimate purposes like SEO auditing, content verification, and site maintenance

---

## ğŸ“„ License

MIT License - feel free to use, modify, and distribute.

---

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit issues and pull requests.

---

**Made with â¤ï¸ and Go**
