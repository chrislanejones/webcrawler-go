# ğŸ•·ï¸ Web Crawler - Cloudflare Buster Edition

![Golang Web Crawler Banner with Spider](Golang-Web-Crawler-Banner.jpg)
A powerful Go-based web crawler with an interactive terminal wizard interface. Features intelligent Cloudflare bypass strategies, comprehensive statistics, and support for HTML, PDF, and DOCX content scanning.

![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=flat&logo=go)
![License](https://img.shields.io/badge/License-MIT-green.svg)

---

## âœ¨ Features

### ğŸ¯ Six Powerful Modes

| Mode                     | Description                                                                |
| ------------------------ | -------------------------------------------------------------------------- |
| **ğŸ”— Find Link**         | Search for specific URLs/links across HTML pages, PDFs, and Word documents |
| **ğŸ“ Find Word/Phrase**  | Search for any text string across all supported content types              |
| **ğŸ’” Broken Link Check** | Scan entire site for 404s, timeouts, and connection errors                 |
| **ğŸ–¼ï¸ Oversized Images**  | Find images exceeding a specified file size threshold                      |
| **ğŸ“„ Page Capture**      | Generate PDFs, screenshots, or CMYK files for every page on the site       |
| **ğŸ—ºï¸ XML Sitemap**       | Generate a standards-compliant XML sitemap by crawling the entire site     |

### ğŸŒ² Path Filtering (Crawl Subsections)

Crawl only a specific section of a website by including the path in your URL:

```
ğŸŒ What site do you want to check?
   (Tip: Include a path like /newsroom/ to only crawl that section)
   â†’ https://www.example.gov/newsroom/news-releases

   ğŸŒ² Detected path: /newsroom/news-releases/
   ğŸ“ Only crawl pages under this path? (Y/n): y
   âœ“ Will only crawl pages under /newsroom/news-releases/
```

This is useful for:

- Crawling only a blog, newsroom, or documentation section
- Avoiding irrelevant pages on large sites
- Faster, more focused crawls

**Smart Archive Detection:** For news/press release sections, the crawler automatically generates year/month archive URLs (e.g., `/newsroom/news-releases/2025/january/`) to discover all articles even when the listing page uses JavaScript pagination.

### ğŸ“„ Page Capture Options

| Format                | Output          | Requirements         |
| --------------------- | --------------- | -------------------- |
| **PDF only**          | `.pdf`          | Chrome/Chromium      |
| **Images only**       | `.png`          | Chrome/Chromium      |
| **Both PDF + Images** | `.pdf` + `.png` | Chrome/Chromium      |
| **CMYK PDF**          | `_cmyk.pdf`     | Chrome + Ghostscript |
| **CMYK TIFF**         | `_cmyk.tiff`    | Chrome + ImageMagick |

### ğŸ—ºï¸ XML Sitemap Generation

Generate a standards-compliant XML sitemap for any website:

| Option               | Description                                                                   |
| -------------------- | ----------------------------------------------------------------------------- |
| **Filename**         | Custom output filename (default: `sitemap.xml`)                               |
| **Change Frequency** | How often pages change: always, hourly, daily, weekly, monthly, yearly, never |
| **Priority**         | Page priority from 0.0 to 1.0 (default: 0.5)                                  |
| **Last Modified**    | Optionally include `<lastmod>` dates from server headers                      |

The generated sitemap follows the [sitemaps.org protocol](https://www.sitemaps.org/protocol.html) and is compatible with all major search engines.

### ğŸ›¡ï¸ Cloudflare Bypass Strategies

The crawler employs multiple techniques to handle bot protection:

- **Alternative Entry Points**: Automatically tests 17+ common pages (`/about`, `/contact`, `/sitemap.xml`, etc.) when the main page is blocked
- **Custom Entry Point**: Specify your own "back door" URL
- **Multi-Phase Crawling**: Start from working pages, then retry blocked pages with established session cookies
- **User Agent Rotation**: Cycles through 5 different browser signatures
- **Session Persistence**: Maintains cookies across requests
- **Exponential Backoff**: Smart retry delays to avoid rate limiting

### ğŸ“Š Comprehensive Statistics

Real-time and final statistics include:

- Pages checked, matches found, errors, blocked pages
- Content breakdown (HTML, PDF, DOCX, images, links)
- Network stats (bytes downloaded, retries, blocked count)
- HTTP status code distribution (2xx, 3xx, 4xx, 5xx)
- Connection error categorization (timeouts, DNS, SSL, refused)
- Performance metrics (pages/second, avg download speed, avg page size)
- Cloudflare bypass stats (retried, recovered, still blocked, recovery rate)

---

## ğŸš€ Quick Start

### Prerequisites

- Go 1.21 or higher
- `pdfcpu` CLI tool (for PDF text extraction)
- Chrome or Chromium (for page capture mode)
- Ghostscript (optional, for CMYK PDF output)
- ImageMagick (optional, for CMYK TIFF output)

### Installation

```bash
# Clone or download the project
git clone <repository-url>
cd webcrawler

# Install dependencies
go mod tidy

# Install pdfcpu for PDF support
go install github.com/pdfcpu/pdfcpu/cmd/pdfcpu@latest
export PATH=$PATH:$(go env GOPATH)/bin

# Install Chrome/Chromium (required for page capture mode)
# Ubuntu/Debian:
sudo apt install chromium-browser
# Or Google Chrome:
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo dpkg -i google-chrome-stable_current_amd64.deb

# Optional: Install Ghostscript (for CMYK PDF)
sudo apt install ghostscript

# Optional: Install ImageMagick (for CMYK TIFF)
sudo apt install imagemagick
```

### Running

```bash
go run main.go
```

The interactive wizard will guide you through the configuration.

---

## ğŸ“– Usage Guide

### Step-by-Step Wizard

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   ğŸ•·ï¸  Web Crawler Wizard  ğŸ•·ï¸                       â•‘
â•‘                        v2.1 - Cloudflare Buster                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŒ What site do you want to check?
   (Tip: Include a path like /newsroom/ to only crawl that section)
   â†’ example.com

ğŸ” Testing connection to https://example.com...
   ğŸ”„ Attempt 1/3 âœ… OK (200) - 245ms latency

ğŸ“‹ What should I check the site for?

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  1. ğŸ”— Find a link on site (HTML, Word, PDF)            â”‚
   â”‚  2. ğŸ“ Find a word/phrase on site (HTML, Word, PDF)     â”‚
   â”‚  3. ğŸ’” Search for broken links                          â”‚
   â”‚  4. ğŸ–¼ï¸  Search for oversized images                     â”‚
   â”‚  5. ğŸ“„ Generate PDF/Image for every page                â”‚
   â”‚  6. ğŸ—ºï¸  Generate XML sitemap                            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Enter choice (1-6): 2

ğŸ“ Enter the word or phrase to search for:
   â†’ privacy policy

âš¡ Max concurrent requests (default 5, max 20): 10

ğŸ”„ Max retries per page (default 3): 3
```

### Path Filtering Example

To crawl only a specific section of a site, include the path in the URL:

```
ğŸŒ What site do you want to check?
   (Tip: Include a path like /newsroom/ to only crawl that section)
   â†’ https://www.governor.virginia.gov/newsroom/news-releases

   ğŸŒ² Detected path: /newsroom/news-releases/
   ğŸ“ Only crawl pages under this path? (Y/n): y
   âœ“ Will only crawl pages under /newsroom/news-releases/

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LAUNCH CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŒ Target:       https://www.governor.virginia.go... â”‚
â”‚  ğŸŒ² Path filter:  /newsroom/news-releases/            â”‚
â”‚  ğŸ“Š Mode:         Page Capture                        â”‚
â”‚  âš¡ Concurrency:  20                                  â”‚
â”‚  ğŸ”„ Max retries:  3                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The crawler will only visit pages whose URL path starts with `/newsroom/news-releases/`, skipping all other sections of the site.

### Page Capture Mode (Option 5)

When you select option 5, you'll see a sub-menu for output format:

```
ğŸ“„ What format do you want to capture?

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  a. ğŸ“‘ PDF only                                         â”‚
   â”‚  b. ğŸ–¼ï¸  Images only (PNG)                                â”‚
   â”‚  c. ğŸ“‘ğŸ–¼ï¸  Both PDF + Images                              â”‚
   â”‚  d. ğŸ¨ CMYK PDF (for print) *                            â”‚
   â”‚  e. ğŸ¨ CMYK TIFF (for InDesign) *                        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   * Requires Ghostscript (d) or ImageMagick (e) installed

   Enter choice (a/b/c/d/e): c
   ğŸ“‘ğŸ–¼ï¸  Will generate both PDFs and PNG screenshots
   ğŸ“ Output folder: ./page_captures/

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PAGE CAPTURE STARTING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ Target: https://example.com/newsroom/                  â”‚
â”‚  ğŸŒ² Path:   /newsroom/                                     â”‚
â”‚  ğŸ“ Output: page_captures_2024-01-15_14-30-00              â”‚
â”‚  ğŸ“‹ Format: PDF + Images                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ’¡ Press 'c' + Enter to cancel and save current progress  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tip:** Press `c` + Enter at any time to stop crawling and keep the files captured so far.

### Sitemap Generation Mode (Option 6)

When you select option 6, you can configure the sitemap output:

```
ğŸ—ºï¸  Sitemap Generation Options

   ğŸ“„ Output filename (default: sitemap.xml): my-sitemap.xml

   ğŸ“… Default change frequency:
      1. always
      2. hourly
      3. daily
      4. weekly (default)
      5. monthly
      6. yearly
      7. never
   Enter choice (1-7): 4
   âœ“ Change frequency: weekly

   â­ Default priority (0.0-1.0, default 0.5): 0.8
   âœ“ Priority: 0.8

   ğŸ• Include last modified date from server? (Y/n): y
   âœ“ Will include Last-Modified dates when available

   ğŸ“ Output file: ./my-sitemap.xml

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SITEMAP GENERATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŒ Target: https://example.com                          â”‚
â”‚  ğŸ“„ Output: my-sitemap.xml                               â”‚
â”‚  ğŸ“… Freq:   weekly                                       â”‚
â”‚  â­ Priority: 0.8                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ—ºï¸  [2m 15s] Found: 142 | Checked: 140 | Errors: 2 | Blocked: 0 | 1.0 p/s

ğŸ“ Generating sitemap XML...
âœ… Sitemap written to: my-sitemap.xml
   ğŸ“Š Total URLs: 140
   ğŸ“¦ File size: 18.5 KB
```

The generated sitemap follows the standard XML format:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>https://example.com/</loc>
    <lastmod>2024-01-15</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>https://example.com/about</loc>
    <lastmod>2024-01-10</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>
  ...
</urlset>
```

### Batch Mode (Process URL List)

Instead of crawling a site, you can capture PDFs from a specific list of URLs by creating a `targets.txt` file:

1. Create a file named `targets.txt` in the project directory
2. Paste your URLs (one per line, or the crawler will extract them automatically)
3. Run with batch mode enabled

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   ğŸ•·ï¸  WEB CRAWLER: BATCH MODE                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‚ Looking for 'targets.txt'...
âœ… Found 47 unique URLs in targets.txt

ğŸš€ Start generating PDFs? (y/n): y

ğŸš€ BATCH CAPTURE STARTING
ğŸ“¦ Links to process: 47
ğŸ“ Saving to: batch_captures_2024-01-15_143022
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Progress: 47/47 pages handled...

ğŸ‰ ALL DONE! Check the 'batch_captures_2024-01-15_143022' folder.
```

This is useful when you:

- Already have a list of specific URLs to capture
- Want to re-capture pages that failed in a previous crawl
- Need to process URLs from an external source (spreadsheet, sitemap, etc.)

### Handling Cloudflare Protection

When Cloudflare blocks the main page:

```
ğŸ” Testing connection to https://protected-site.com...
   ğŸ”„ Attempt 1/3 ğŸ›¡ï¸  CLOUDFLARE DETECTED (403)
   â³ Waiting 3s before retry with different headers...
   ğŸ”„ Attempt 2/3 ğŸ›¡ï¸  CLOUDFLARE DETECTED (403)
   â³ Waiting 6s before retry with different headers...
   ğŸ”„ Attempt 3/3 ğŸ›¡ï¸  CLOUDFLARE DETECTED (403)

   ğŸ›¡ï¸  Cloudflare/Bot protection detected on main page!
   ğŸ’¡ Let's try some alternative entry points...

   Testing common entry points...

   [ 1/17] Testing /about                âœ… WORKS!
   [ 2/17] Testing /about-us             âŒ Failed
   [ 3/17] Testing /contact              âœ… WORKS!
   [ 4/17] Testing /contact-us           ğŸ›¡ï¸  Blocked
   ...

   âœ… Found 2 working entry point(s)!
   ğŸ”„ Will start from these and retry blocked pages later
```

---

## ğŸ“Š Output

### Live Statistics

During crawling, you'll see real-time updates with a progress bar:

```
ğŸ“Š [2m 15s] Pages: 142 | Matches: 8 | Errors: 3 | Blocked: 2 (Queue: 1, Recovered: 1) | 1.1 p/s | 45.2 KB/s
```

For page capture mode, you'll see a live progress bar with the current page being processed:

```
â ¹ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  60% â”‚ â± 4m 30s â”‚ ğŸ“‘ 45 captured â”‚ â³ 30 pending â”‚ âŒ 2 â”‚ 0.8/s
   â†’ .../newsroom/news-releases/2025/december/name-1072620-en.html
```

### Final Report

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      ğŸ“Š FINAL STATISTICS ğŸ“Š                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘  â±ï¸  Total Time:           5m 32s                                  â•‘
â•‘  ğŸ“„ Pages Checked:         347                                    â•‘
â•‘  âœ… Matches Found:         23                                     â•‘
â•‘  ğŸ“ Results File:          results-search-2024-01-15_14-30-00.csv â•‘
â•‘                                                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                      ğŸ”¬ CONTENT BREAKDOWN                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“ HTML Pages:            312                                    â•‘
â•‘  ğŸ“• PDF Documents:         28                                     â•‘
â•‘  ğŸ“˜ Word Documents:        7                                      â•‘
â•‘  ğŸ–¼ï¸  Images Checked:        0                                      â•‘
â•‘  ğŸ”— Links Checked:         0                                      â•‘
â•‘  â­ï¸  Skipped (External):    156                                    â•‘
...
```

For page capture mode:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  ğŸ“Š PAGE CAPTURE COMPLETE ğŸ“Š                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘  â±ï¸  Total Time:           4m 30s                                  â•‘
â•‘  ğŸ“„ Pages Visited:         180                                    â•‘
â•‘  ğŸ“‘ PDFs Generated:        152                                    â•‘
â•‘  ğŸ–¼ï¸  Images Generated:      152                                    â•‘
â•‘  âŒ Errors:                9                                      â•‘
â•‘  ğŸ“ Output Directory:      page_captures_2024-01-15_14-30-00      â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

For sitemap generation mode:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  ğŸ—ºï¸  SITEMAP GENERATION COMPLETE  ğŸ—ºï¸               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘  â±ï¸  Total Time:           2m 15s                                  â•‘
â•‘  ğŸ“„ URLs in Sitemap:       140                                    â•‘
â•‘  ğŸ” Pages Checked:         145                                    â•‘
â•‘  âŒ Errors:                3                                      â•‘
â•‘  ğŸ›¡ï¸  Blocked:               2                                      â•‘
â•‘  â­ï¸  Skipped (filtered):    0                                      â•‘
â•‘                                                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                      ğŸ“ OUTPUT FILE                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“„ Filename:              sitemap.xml                            â•‘
â•‘  ğŸ“… Change Frequency:      weekly                                 â•‘
â•‘  â­ Priority:              0.5                                    â•‘
â•‘  ğŸ• Include Last Modified: Yes                                    â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš¡ Performance: 1.07 pages/second
```

### CSV Results

Results are saved to timestamped CSV files:

**Search Mode:**

```csv
URL,ContentType,FoundIn,Target,Timestamp
https://example.com/page1,text/html,HTML,privacy policy,2024-01-15T14:32:45Z
https://example.com/docs/terms.pdf,application/pdf,PDF,privacy policy,2024-01-15T14:33:12Z
```

**Broken Links Mode:**

```csv
BrokenURL,FoundOnPage,StatusCode,Error,Timestamp
https://example.com/old-page,https://example.com/links,404,Not Found,2024-01-15T14:32:45Z
```

**Oversized Images Mode:**

```csv
ImageURL,FoundOnPage,SizeKB,ContentType,Timestamp
https://example.com/hero.jpg,https://example.com/,2048,image/jpeg,2024-01-15T14:32:45Z
```

**Page Capture Mode:**

Files are saved directly to a timestamped folder (e.g., `pdf_captures_2024-01-15_14-30-00/`):

- `index.pdf` / `index.png` - Homepage
- `about.pdf` / `about.png` - About page
- `contact_us.pdf` / `contact_us.png` - Contact page
- etc.

**Sitemap Mode:**

An XML file is generated (e.g., `sitemap.xml`) containing all discovered URLs with optional metadata.

---

## âš™ï¸ Configuration Options

| Option               | Default | Description                                              |
| -------------------- | ------- | -------------------------------------------------------- |
| Concurrency          | 5       | Number of concurrent requests (max 20)                   |
| Max Retries          | 3       | Retry attempts per page on failure                       |
| Retry Delay          | 2s      | Base delay between retries (increases exponentially)     |
| Blocked Retry Passes | 3       | Number of passes to retry blocked pages                  |
| Image Size Threshold | 500KB   | Threshold for oversized image detection                  |
| Path Filter          | (none)  | Only crawl URLs starting with this path (e.g., `/blog/`) |
| Ignore Query Params  | No      | Treat URLs with different query strings as the same page |
| Page Timeout         | 180s    | Max time to wait for a page to render (Page Capture)     |

### Ignore Query Parameters

Some websites use cache-busting or tracking query parameters that create duplicate URLs pointing to the same content:

```
https://example.com/page.html?cache=abc123
https://example.com/page.html?cache=def456
https://example.com/page.html?tracking=xyz
```

When **Ignore Query Params** is enabled, the crawler treats all of these as the same page (`https://example.com/page.html`) and only captures it once. This prevents duplicate files and speeds up crawling.

**When to use:**
- Sites with cache-busting query parameters
- Sites with tracking/analytics parameters in URLs
- News sites that add random query strings to links

### Sitemap-Specific Options

| Option          | Default       | Description                                     |
| --------------- | ------------- | ----------------------------------------------- |
| Filename        | `sitemap.xml` | Output filename for the generated sitemap       |
| Change Freq     | `weekly`      | How often pages typically change                |
| Priority        | `0.5`         | Default priority for all URLs (0.0 - 1.0)       |
| Include LastMod | `true`        | Include Last-Modified dates from server headers |

---

## ğŸš¨ Error Detection

### Network Errors

| Icon | Error Type         | Description                          |
| ---- | ------------------ | ------------------------------------ |
| â±ï¸   | Timeout            | Server not responding                |
| ğŸš«   | Connection Refused | Server actively refusing connections |
| ğŸŒ   | DNS Error          | Domain not found                     |
| ğŸ”’   | SSL/TLS Error      | Certificate validation failed        |

### HTTP Status Codes

| Code    | Handling                                   |
| ------- | ------------------------------------------ |
| 200-299 | Success - content processed                |
| 300-399 | Redirects followed (up to 10)              |
| 403/503 | Bot protection detected - queued for retry |
| 429     | Rate limited - backed off and retried      |
| 404     | Not found - logged as error                |
| 5xx     | Server error - retried                     |

### Bot Protection Detection

Automatically identifies:

- Cloudflare ("Checking your browser...", "Ray ID")
- Incapsula/Imperva
- PerimeterX
- Sucuri
- Generic CAPTCHA challenges
- DDoS protection pages

---

## ğŸ“ Project Structure

```
webcrawler/
â”œâ”€â”€ main.go                      # Interactive wizard & entry point
â”œâ”€â”€ go.mod                       # Go module definition
â”œâ”€â”€ go.sum                       # Dependency checksums
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ tmp/                     # Temporary files for PDF processing
â””â”€â”€ internal/
    â”œâ”€â”€ crawler/
    â”‚   â”œâ”€â”€ crawler.go           # Core crawling logic & statistics
    â”‚   â”œâ”€â”€ pdfcapture.go        # Page capture with Chrome/PDF/CMYK
    â”‚   â””â”€â”€ sitemap.go           # XML sitemap generation
    â””â”€â”€ parser/
        â”œâ”€â”€ docx.go              # Word document parser
        â””â”€â”€ pdf.go               # PDF text extractor
```

---

## ğŸ”§ Troubleshooting

### "pdfcpu: command not found"

```bash
go install github.com/pdfcpu/pdfcpu/cmd/pdfcpu@latest
export PATH=$PATH:$(go env GOPATH)/bin
```

### "google-chrome: executable file not found" (Page Capture Mode)

```bash
# Install Chromium
sudo apt install chromium-browser

# Or install Google Chrome
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo dpkg -i google-chrome-stable_current_amd64.deb
sudo apt --fix-broken install -y
```

### "ghostscript (gs) not found" (CMYK PDF)

```bash
sudo apt install ghostscript
```

### "imagemagick not found" (CMYK TIFF)

```bash
sudo apt install imagemagick
```

### "context deadline exceeded" (Page Capture Mode)

This means a page took longer than 180 seconds to render. Options:

- Ignore it (a few timeouts are normal for slow pages)
- Reduce concurrency to give Chrome more resources
- Some pages with heavy JavaScript may always timeout

### Path filter not finding all pages

If the crawler isn't finding all pages in a section:

- The site may use JavaScript-loaded content that doesn't expose links in the DOM
- For news/press release sections, the crawler auto-generates year/month archive URLs
- Try entering a more specific path or a known archive URL directly
- Some sites use infinite scroll or AJAX pagination that can't be fully crawled

### PDFs show only header/footer, no body content

This happens when the page body is loaded via JavaScript/AJAX after the initial page load:

- The crawler now waits for body content to stabilize before capturing
- If still seeing empty bodies, the site may use a complex loading pattern
- Try reducing concurrency to give Chrome more time to render
- Some heavily JavaScript-dependent pages may not capture well

### High blocked page count

- Reduce concurrency: `âš¡ Max concurrent requests: 3`
- Increase retry count
- Try running at a different time
- Some sites genuinely require JavaScript execution

### Rate limiting (429 errors)

The crawler automatically backs off, but you can:

- Reduce concurrency
- Increase the built-in delay (edit `time.Sleep(50 * time.Millisecond)` in `crawler.go`)

### SSL certificate errors

The crawler skips certificate verification by default (`InsecureSkipVerify: true`). This handles self-signed certs but be aware of the security implications.

### Empty sitemap generated

If the sitemap has no URLs:

- Check if the site is accessible and not blocking the crawler
- Verify the path filter isn't too restrictive
- The site might be heavily JavaScript-dependent (sitemap mode only crawls static HTML links)
- Try with lower concurrency to avoid rate limiting

---

## ğŸ› ï¸ Building

```bash
# Build for current platform
go build -o webcrawler main.go

# Cross-compile for Linux
GOOS=linux GOARCH=amd64 go build -o webcrawler-linux main.go

# Cross-compile for Windows
GOOS=windows GOARCH=amd64 go build -o webcrawler.exe main.go

# Cross-compile for macOS
GOOS=darwin GOARCH=amd64 go build -o webcrawler-mac main.go
```

---

## ğŸ“ Dependencies

- [golang.org/x/net](https://pkg.go.dev/golang.org/x/net) - HTML parsing
- [baliance.com/gooxml](https://github.com/baliance/gooxml) - DOCX parsing
- [pdfcpu](https://github.com/pdfcpu/pdfcpu) - PDF text extraction (external CLI)
- [chromedp](https://github.com/chromedp/chromedp) - Chrome DevTools Protocol (for page capture)
- [charmbracelet/huh](https://github.com/charmbracelet/huh) - Interactive terminal forms (wizard interface)

---

## âš ï¸ Legal & Ethical Considerations

- Always respect `robots.txt` (manual check recommended)
- Be mindful of rate limits and server load
- Only crawl sites you have permission to access
- This tool is for legitimate purposes like SEO auditing, content verification, and site maintenance

---

## ğŸ“„ License

MIT License - feel free to use, modify, and distribute.

---

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit issues and pull requests.

---

**Made with â¤ï¸ and Go**
